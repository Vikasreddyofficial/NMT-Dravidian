{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "863c7787",
   "metadata": {},
   "source": [
    "#Model Trainig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e6cb1f-085e-45d8-b778-5c15f46146a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 17:01:18.050467: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-06 17:01:18.062478: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746531078.076098  322489 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746531078.080205  322489 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746531078.090613  322489 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746531078.090628  322489 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746531078.090630  322489 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746531078.090631  322489 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-06 17:01:18.094332: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Finance DataFrame shape: (32334, 2)\n",
      "Sample finance data:\n",
      "                                                Tamil  \\\n",
      "0  கடந்த நிதியாண்டு முழுவதும் இந்நிறுவனத்தின் வரு...   \n",
      "1  முதலிடத்தில் இருப்பது கேட்ஸ் 2006 ல் மைக்ரோசா...   \n",
      "2  கடந்த ஆண்டு வழங்கப்பட்ட தொகையுடன் ஒப்பிட்டால் ...   \n",
      "3  கருப்புப் பணத்திற்கு எதிராக டிஜிட்டல் பரிவர்த்...   \n",
      "4  இந்த நவீனமயமாக்கல் மூலமாக நிறுவன வள திட்டமிடல்...   \n",
      "\n",
      "                                              Telugu  \n",
      "0  గత ఆర్థిక సంవత్సరంలో కంపెనీ ఆదాయం రూ .335.53 క...  \n",
      "1  2006 లో గేట్స్ మైక్రోసాఫ్ట్ నుండి బయలుదేరినప్ప...  \n",
      "2  గత సంవత్సరం చెల్లించిన మొత్తంతో పోలిస్తే ఇది 3...  \n",
      "3  బ్లాక్ మనీకి వ్యతిరేకంగా డిజిటల్ లావాదేవీలను ప...  \n",
      "4  ఈ ఆధునీకరణతో, ERP పరిష్కారాలను అటువంటి కార్యకల...  \n",
      "Finance train dataset size: 29100\n",
      "Finance test dataset size: 3234\n",
      "mBART - Tokenizer vocab size: 250054\n",
      "mBART - Model output vocab size: 250054\n",
      "mBART - Vocab sizes match, no adjustment needed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6369cca7c7b64a1f966ca846a21782e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:4109: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6a3cd3b02d4681b18716bd8ed755ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3234 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finance - Tokenized train sample: {'input_ids': [250045, 135813, 16957, 1886, 41095, 3049, 90382, 2695, 7747, 16577, 2195, 18472, 28184, 4276, 124584, 142635, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [250044, 15413, 12391, 39639, 4575, 123555, 2913, 22262, 57836, 10479, 39438, 6001, 108471, 13764, 24395, 95789, 58594, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n",
      "Fine-tuning mBART on finance data (Telugu to Tamil)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9095' max='9095' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9095/9095 1:31:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.103000</td>\n",
       "      <td>1.032806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.934800</td>\n",
       "      <td>0.991043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.985544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.771600</td>\n",
       "      <td>0.989645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.711200</td>\n",
       "      <td>0.998757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mBART Finance - Saved tokenizer vocab size: 250054\n",
      "mBART Finance - Saved model output vocab size: 250054\n",
      "Tokenized Input IDs: [[250045, 167493, 198661, 1296, 24663, 2127, 148763, 24663, 239555, 18668, 5, 2]]\n",
      "Raw Output IDs: [2, 250044, 115828, 1962, 57494, 194272, 80087, 105723, 18890, 95591, 142240, 5, 2]\n",
      "Decoded with special tokens: </s>ta_IN வங்கிக் கணக்கில் முதலீடு செய்ய விரும்புகிறேன்.</s>\n",
      "mBART Finance Translation (Telugu to Tamil): வங்கிக் கணக்கில் முதலீடு செய்ய விரும்புகிறேன்.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import MBart50TokenizerFast, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "import uuid\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load finance dataset\n",
    "df = pd.read_csv('./Finance_Data/Finance_Data_Cleaned.csv') \n",
    "df_finance = df.copy()\n",
    "df_finance.drop(['Unnamed: 0.1', 'Unnamed: 0'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "print(\"Finance DataFrame shape:\", df_finance.shape)\n",
    "print(\"Sample finance data:\\n\", df_finance.head(5))\n",
    "\n",
    "\n",
    "df_finance = df_finance.rename(columns={\"Tamil\": \"ta\", \"Telugu\": \"te\"})\n",
    "\n",
    "\n",
    "dataset = Dataset.from_pandas(df_finance)\n",
    "split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "finance_dataset = DatasetDict({\"train\": split_dataset[\"train\"], \"test\": split_dataset[\"test\"]})\n",
    "print(\"Finance train dataset size:\", len(finance_dataset[\"train\"]))\n",
    "print(\"Finance test dataset size:\", len(finance_dataset[\"test\"]))\n",
    "\n",
    "# Load pre-trained mBART model and tokenizer from previous training\n",
    "MBART_MODEL_PATH = \"./mbart_finetuned_te_to_ta\"\n",
    "mbart_tokenizer = MBart50TokenizerFast.from_pretrained(MBART_MODEL_PATH, src_lang=\"te_IN\", tgt_lang=\"ta_IN\")  # Changed src to te_IN, tgt to ta_IN\n",
    "mbart_model = AutoModelForSeq2SeqLM.from_pretrained(MBART_MODEL_PATH).to(device)\n",
    "\n",
    "# Verify vocab sizes\n",
    "mbart_vocab_size_tokenizer = len(mbart_tokenizer)\n",
    "mbart_vocab_size_model = mbart_model.get_output_embeddings().weight.size(0)\n",
    "print(\"mBART - Tokenizer vocab size:\", mbart_vocab_size_tokenizer)\n",
    "print(\"mBART - Model output vocab size:\", mbart_vocab_size_model)\n",
    "\n",
    "\n",
    "if mbart_vocab_size_tokenizer != mbart_vocab_size_model:\n",
    "    print(f\"Warning: Vocab size mismatch (Tokenizer: {mbart_vocab_size_tokenizer}, Model: {mbart_vocab_size_model}). Adjusting model embeddings.\")\n",
    "    mbart_model.resize_token_embeddings(mbart_vocab_size_tokenizer)\n",
    "    print(\"Post-resize model vocab size:\", mbart_model.get_output_embeddings().weight.size(0))\n",
    "else:\n",
    "    print(\"mBART - Vocab sizes match, no adjustment needed.\")\n",
    "\n",
    "# Preprocessing function\n",
    "def mbart_preprocess_function(examples):\n",
    "    inputs = [te_text for te_text in examples[\"te\"]] \n",
    "    targets = [ta_text for ta_text in examples[\"ta\"]] \n",
    "    model_inputs = mbart_tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    with mbart_tokenizer.as_target_tokenizer():\n",
    "        labels = mbart_tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\").input_ids\n",
    "    labels = [[-100 if token == mbart_tokenizer.pad_token_id else token for token in seq] for seq in labels]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "finance_tokenized_datasets = finance_dataset.map(\n",
    "    mbart_preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    remove_columns=[\"ta\", \"te\"]\n",
    ")\n",
    "print(\"Finance - Tokenized train sample:\", finance_tokenized_datasets[\"train\"][0])\n",
    "\n",
    "# Training arguments for fine-tuning\n",
    "finance_training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mbart_finetuned_finance_te2ta\",  \n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=5,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    logging_steps=200,\n",
    "    save_steps=2000\n",
    ")\n",
    "\n",
    "# Data collator and trainer\n",
    "finance_data_collator = DataCollatorForSeq2Seq(mbart_tokenizer, model=mbart_model)\n",
    "finance_trainer = Seq2SeqTrainer(\n",
    "    model=mbart_model,\n",
    "    args=finance_training_args,\n",
    "    train_dataset=finance_tokenized_datasets[\"train\"],\n",
    "    eval_dataset=finance_tokenized_datasets[\"test\"],\n",
    "    tokenizer=mbart_tokenizer,\n",
    "    data_collator=finance_data_collator\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Fine-tuning mBART on finance data (Telugu to Tamil)...\")\n",
    "finance_trainer.train()\n",
    "\n",
    "\n",
    "finance_trainer.save_model(\"./mbart_finetuned_finance_te2ta\")\n",
    "mbart_tokenizer.save_pretrained(\"./mbart_finetuned_finance_te2ta\")\n",
    "\n",
    "\n",
    "mbart_finance_model = AutoModelForSeq2SeqLM.from_pretrained(\"./mbart_finetuned_finance_te2ta\").to(device)\n",
    "mbart_finance_tokenizer = MBart50TokenizerFast.from_pretrained(\"./mbart_finetuned_finance_te2ta\", src_lang=\"te_IN\", tgt_lang=\"ta_IN\") \n",
    "print(\"mBART Finance - Saved tokenizer vocab size:\", len(mbart_finance_tokenizer))\n",
    "print(\"mBART Finance - Saved model output vocab size:\", mbart_finance_model.get_output_embeddings().weight.size(0))\n",
    "\n",
    "# Test translation with debugging\n",
    "def mbart_translate_finance_text(input_text, debug=False):\n",
    "    inputs = mbart_finance_tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True, padding=True).to(device)\n",
    "    if debug:\n",
    "        print(\"Tokenized Input IDs:\", inputs[\"input_ids\"].tolist())\n",
    "    outputs = mbart_finance_model.generate(\n",
    "        **inputs,\n",
    "        max_length=256,\n",
    "        min_length=10,\n",
    "        num_beams=5,\n",
    "        early_stopping=False,\n",
    "        length_penalty=1.0,\n",
    "        no_repeat_ngram_size=2,\n",
    "        forced_bos_token_id=mbart_finance_tokenizer.lang_code_to_id[\"ta_IN\"]  \n",
    "    )\n",
    "    if debug:\n",
    "        print(\"Raw Output IDs:\", outputs[0].tolist())\n",
    "        print(\"Decoded with special tokens:\", mbart_finance_tokenizer.decode(outputs[0], skip_special_tokens=False))\n",
    "    decoded_output = mbart_finance_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return decoded_output.strip()\n",
    "\n",
    "# Test with sample\n",
    "input_text = \"బ్యాంకు ఖాతాలో పెట్టుబడి పెట్టాలనుకుంటున్నాను.\"  # \"I want to invest in a bank account.\"\n",
    "translated_text = mbart_translate_finance_text(input_text, debug=True)\n",
    "print(\"mBART Finance Translation (Telugu to Tamil):\", translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaf5837",
   "metadata": {},
   "source": [
    "#Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c100e65a-cd5b-4e6f-8efd-1fb6089acf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using device: cuda\n",
      "INFO:__main__:Loading model configuration...\n",
      "INFO:__main__:Loading model and tokenizer...\n",
      "INFO:__main__:Loading dataset...\n",
      "INFO:__main__:Generating translations for 800 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: ['index', 'Tamil', 'Telugu', 'tamil_vocab_score', 'telugu_vocab_score', 'reason', 'similarity_score']\n",
      "Dataset shape: (838, 7)\n",
      "First few rows:\n",
      "   index                                              Tamil  \\\n",
      "0     64  ஒரு மாத கடனுக்கு 8.45%, மூன்று மாதங்களுக்கு 8....   \n",
      "1     88               கிரெடிட் கார்டில் பல நன்மைகள் உள்ளன.   \n",
      "2    108  2016 ஆம் ஆண்டில், இது 62.7 பில்லியன் டாலராகவும...   \n",
      "3    118  ஜூலை 1 முதல் நாடு முழுவதும் ஜிஎஸ்டி செயல்படுத்...   \n",
      "4    135  அமெரிக்காவில் வட்டி விகிதங்கள் அதிகரித்தால், உ...   \n",
      "\n",
      "                                              Telugu  tamil_vocab_score  \\\n",
      "0  ఒక నెల కాలావ‌ధి రుణాల‌కు 8.45%, మూడు నెల‌ల కాల...           0.800000   \n",
      "1     క్రెడిట్ కార్డు వ‌ల్ల చాలా ప్ర‌యోజ‌నాలున్నాయి.           1.000000   \n",
      "2  2016 సంవత్సరంలో 62.7 బిలియన్‌ డాలర్లు, 2017 65...           0.733333   \n",
      "3  జులై 1 నుంచి దేశ‌వ్యాప్తంగా జీఎస్టీ అమ‌లు అవ‌న...           0.923077   \n",
      "4  అమెరికాలో వ‌డ్డీ రేట్లు పెరిగితే బంగారం ధ‌ర‌లు...           1.000000   \n",
      "\n",
      "   telugu_vocab_score                                             reason  \\\n",
      "0            0.645833                          telugu_low_vocab_coverage   \n",
      "1            0.666667                          telugu_low_vocab_coverage   \n",
      "2            0.666667  tamil_low_vocab_coverage,telugu_low_vocab_cove...   \n",
      "3            0.571429                          telugu_low_vocab_coverage   \n",
      "4            0.583333                          telugu_low_vocab_coverage   \n",
      "\n",
      "   similarity_score  \n",
      "0          0.900640  \n",
      "1          0.931336  \n",
      "2          0.926532  \n",
      "3          0.942610  \n",
      "4          0.965044  \n",
      "Using Telugu column: Telugu\n",
      "Using Tamil column: Tamil\n",
      "Test dataset size: 838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating translations:   0%|                          | 0/800 [00:00<?, ?it/s]/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:554: UserWarning: `num_beams` is set to None - defaulting to 1.\n",
      "  warnings.warn(\"`num_beams` is set to None - defaulting to 1.\", UserWarning)\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/utils.py:1283: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Generating translations: 100%|████████████████| 800/800 [03:24<00:00,  3.91it/s]\n",
      "INFO:__main__:Computing BLEU score...\n",
      "INFO:__main__:BLEU Score: 61.09\n",
      "INFO:__main__:BLEU results saved to ./Finance_Data/results_reverse/bleu_evaluation_results_te2ta.csv\n",
      "INFO:__main__:Computing BERTScore...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d23abda47b4dafbd5f20127b92bb26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c89d55766647da985e4d81a422c854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:BERTScore F1: 0.8850\n",
      "INFO:__main__:BERTScore results saved to ./Finance_Data/results_reverse/bertscore_evaluation_results_te2ta.csv\n",
      "INFO:__main__:Filtered results (BERTScore F1 >= 0.85) saved to ./Finance_Data/results_reverse/filtered_sentences_te2ta.csv\n",
      "INFO:__main__:Computing chrF++ score...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.29 seconds, 618.71 sentences/sec\n",
      "Filtered dataset size: 568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:chrF++ Score: 73.30\n",
      "INFO:__main__:chrF++ results saved to ./Finance_Data/results_reverse/chrf_evaluation_results_te2ta.csv\n",
      "INFO:__main__:Computing TER score...\n",
      "INFO:__main__:TER Score: 133.78\n",
      "INFO:__main__:TER results saved to ./Finance_Data/results_reverse/ter_evaluation_results_te2ta.csv\n",
      "INFO:__main__:Tokenized Input IDs: [[250045, 24147, 1296, 4, 22735, 24722, 91064, 32, 2]]\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:554: UserWarning: `num_beams` is set to None - defaulting to 1.\n",
      "  warnings.warn(\"`num_beams` is set to None - defaulting to 1.\", UserWarning)\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "INFO:__main__:Raw Output IDs: [2, 250044, 39507, 66705, 4, 29947, 128251, 37961, 19238, 32, 2]\n",
      "INFO:__main__:Decoded with special tokens: </s>ta_IN ஹலோ, எப்படி இருக்கிறாய் நீ?</s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EVALUATION SUMMARY\n",
      "==================================================\n",
      "Number of samples: 800\n",
      "Filtered samples (BERTScore F1 >= 0.85): 568\n",
      "BLEU Score: 61.09\n",
      "chrF++ Score: 73.30\n",
      "TER Score: 133.78 (lower is better)\n",
      "BERTScore F1: 0.8850\n",
      "==================================================\n",
      "\n",
      "Test Translation:\n",
      "Source (Telugu): హలో, మీరు ఎలా ఉన్నారు?\n",
      "Target (Tamil): ஹலோ, எப்படி இருக்கிறாய் நீ?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bleu': 61.090916971845274,\n",
       " 'chrf': 73.30075376298603,\n",
       " 'ter': 133.77926421404683,\n",
       " 'bertscore': 0.8850445747375488}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration, AutoConfig, AutoModelForSeq2SeqLM\n",
    "from sacrebleu import corpus_bleu, corpus_chrf, corpus_ter\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "MODEL_PATH = \"./mbart_finetuned_finance_te2ta\"  \n",
    "DATASET_PATH = \"./Finance_Data/testing.csv\"  \n",
    "NUM_SAMPLES = 800  \n",
    "SIMILARITY_THRESHOLD = 0.85  \n",
    "\n",
    "# Output paths\n",
    "BLEU_OUTPUT_PATH = \"./Finance_Data/results_reverse/bleu_evaluation_results_te2ta.csv\"\n",
    "BERTSCORE_OUTPUT_PATH = \"./Finance_Data/results_reverse/bertscore_evaluation_results_te2ta.csv\"\n",
    "CHRF_OUTPUT_PATH = \"./Finance_Data/results_reverse/chrf_evaluation_results_te2ta.csv\"\n",
    "TER_OUTPUT_PATH = \"./Finance_Data/results_reverse/ter_evaluation_results_te2ta.csv\"\n",
    "FILTERED_OUTPUT_PATH = \"./Finance_Data/results_reverse/filtered_sentences_te2ta.csv\"\n",
    "\n",
    "# Load the model configuration \n",
    "logger.info(\"Loading model configuration...\")\n",
    "config = AutoConfig.from_pretrained(MODEL_PATH)\n",
    "if hasattr(config, 'generation_config'):\n",
    "    if config.generation_config.early_stopping is None:\n",
    "        config.generation_config.early_stopping = True\n",
    "else:\n",
    "    config.early_stopping = True\n",
    "\n",
    "\n",
    "logger.info(\"Loading model and tokenizer...\")\n",
    "try:\n",
    "    mbart_saved_model = MBartForConditionalGeneration.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        config=config\n",
    "    ).to(device)\n",
    "except:\n",
    "    mbart_saved_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        config=config\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "if hasattr(mbart_saved_model, 'generation_config'):\n",
    "    mbart_saved_model.generation_config.early_stopping = True\n",
    "\n",
    "\n",
    "mbart_saved_tokenizer = MBart50TokenizerFast.from_pretrained(MODEL_PATH, src_lang=\"te_IN\", tgt_lang=\"ta_IN\")\n",
    "\n",
    "\n",
    "logger.info(\"Loading dataset...\")\n",
    "df = pd.read_csv(DATASET_PATH, encoding='utf-8')\n",
    "print(f\"Dataset columns: {df.columns.tolist()}\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"First few rows:\\n{df.head()}\")\n",
    "\n",
    "\n",
    "telugu_col = 'Telugu' \n",
    "tamil_col = 'Tamil'   \n",
    "\n",
    "if telugu_col not in df.columns or tamil_col not in df.columns:\n",
    "    raise ValueError(f\"CSV must contain 'Telugu' and 'Tamil' columns. Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "print(f\"Using Telugu column: {telugu_col}\")\n",
    "print(f\"Using Tamil column: {tamil_col}\")\n",
    "\n",
    "\n",
    "df = df[[telugu_col, tamil_col]].dropna()\n",
    "\n",
    "df[telugu_col] = df[telugu_col].astype(str).replace(['nan', 'NaN', ''], '')\n",
    "df[tamil_col] = df[tamil_col].astype(str).replace(['nan', 'NaN', ''], '')\n",
    "\n",
    "df = df[(df[telugu_col] != '') & (df[tamil_col] != '')]\n",
    "df = df.rename(columns={telugu_col: 'telugu_sentence', tamil_col: 'tamil_sentence'})\n",
    "test_dataset = Dataset.from_pandas(df)\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "def indic_tokenize_text(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "    return ' '.join(indic_tokenize.trivial_tokenize(text, lang='ta'))\n",
    "\n",
    "\n",
    "def mbart_translate_text(input_text, debug=False):\n",
    "    if not isinstance(input_text, str) or not input_text.strip():\n",
    "        return \"\"\n",
    "    inputs = mbart_saved_tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True, padding=True).to(device)\n",
    "    if debug:\n",
    "        logger.info(f\"Tokenized Input IDs: {inputs['input_ids'].tolist()}\")\n",
    "    outputs = mbart_saved_model.generate(\n",
    "        **inputs,\n",
    "        max_length=256,\n",
    "        min_length=10,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        length_penalty=1.2,\n",
    "        no_repeat_ngram_size=3,\n",
    "        forced_bos_token_id=mbart_saved_tokenizer.lang_code_to_id[\"ta_IN\"]  # Updated to Tamil\n",
    "    )\n",
    "    if debug:\n",
    "        logger.info(f\"Raw Output IDs: {outputs[0].tolist()}\")\n",
    "        logger.info(f\"Decoded with special tokens: {mbart_saved_tokenizer.decode(outputs[0], skip_special_tokens=False)}\")\n",
    "    decoded_output = mbart_saved_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return decoded_output.strip()\n",
    "\n",
    "\n",
    "def generate_translations(dataset, num_samples=NUM_SAMPLES):\n",
    "    sources = []\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    test_data = dataset.select(range(min(num_samples, len(dataset))))\n",
    "    logger.info(f\"Generating translations for {len(test_data)} samples\")\n",
    "    \n",
    "    for example in tqdm(test_data, desc=\"Generating translations\"):\n",
    "        input_text = example[\"telugu_sentence\"] \n",
    "        reference = example[\"tamil_sentence\"]   \n",
    "        \n",
    "        try:\n",
    "            hypothesis = mbart_translate_text(input_text, debug=False)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error translating '{input_text}': {e}\")\n",
    "            hypothesis = \"\"\n",
    "            \n",
    "        sources.append(input_text)\n",
    "        references.append(reference)\n",
    "        hypotheses.append(hypothesis)\n",
    "    \n",
    "    return test_data, sources, references, hypotheses\n",
    "\n",
    "# Compute BLEU score\n",
    "def compute_bleu(test_data, sources, references, hypotheses):\n",
    "    logger.info(\"Computing BLEU score...\")\n",
    "    \n",
    "    tokenized_hypotheses = [indic_tokenize_text(hyp) for hyp in hypotheses]\n",
    "    tokenized_references = [[indic_tokenize_text(ref)] for ref in references]\n",
    "    \n",
    "    bleu = corpus_bleu(tokenized_hypotheses, tokenized_references, tokenize='none')\n",
    "    bleu_score = bleu.score\n",
    "    logger.info(f\"BLEU Score: {bleu_score:.2f}\")\n",
    "    \n",
    "    results_df = pd.DataFrame({\n",
    "        \"telugu_sentence\": sources,\n",
    "        \"tamil_sentence\": references,\n",
    "        \"tamil_hypothesis\": hypotheses,\n",
    "        \"bleu_score\": [bleu_score] * len(sources)\n",
    "    })\n",
    "    results_df.to_csv(BLEU_OUTPUT_PATH, index=False, encoding='utf-8')\n",
    "    logger.info(f\"BLEU results saved to {BLEU_OUTPUT_PATH}\")\n",
    "    \n",
    "    return bleu_score\n",
    "    \n",
    "# Compute chrF++ score\n",
    "def compute_chrf(test_data, sources, references, hypotheses):\n",
    "    logger.info(\"Computing chrF++ score...\")\n",
    "    \n",
    "    refs_list = [[ref] for ref in references]\n",
    "    chrf = corpus_chrf(hypotheses, refs_list, char_order=6, word_order=2, beta=2)\n",
    "    chrf_score = chrf.score\n",
    "    logger.info(f\"chrF++ Score: {chrf_score:.2f}\")\n",
    "    \n",
    "    results_df = pd.DataFrame({\n",
    "        \"telugu_sentence\": sources,\n",
    "        \"tamil_sentence\": references,\n",
    "        \"tamil_hypothesis\": hypotheses,\n",
    "        \"chrf_score\": [chrf_score] * len(sources)\n",
    "    })\n",
    "    results_df.to_csv(CHRF_OUTPUT_PATH, index=False, encoding='utf-8')\n",
    "    logger.info(f\"chrF++ results saved to {CHRF_OUTPUT_PATH}\")\n",
    "    \n",
    "    return chrf_score\n",
    "    \n",
    "# Compute TER score\n",
    "def compute_ter(test_data, sources, references, hypotheses):\n",
    "    logger.info(\"Computing TER score...\")\n",
    "    \n",
    "    refs_list = [[ref] for ref in references]\n",
    "    ter = corpus_ter(hypotheses, refs_list)\n",
    "    ter_score = ter.score\n",
    "    logger.info(f\"TER Score: {ter_score:.2f}\")\n",
    "    \n",
    "    results_df = pd.DataFrame({\n",
    "        \"telugu_sentence\": sources,\n",
    "        \"tamil_sentence\": references,\n",
    "        \"tamil_hypothesis\": hypotheses,\n",
    "        \"ter_score\": [ter_score] * len(sources)\n",
    "    })\n",
    "    results_df.to_csv(TER_OUTPUT_PATH, index=False, encoding='utf-8')\n",
    "    logger.info(f\"TER results saved to {TER_OUTPUT_PATH}\")\n",
    "    \n",
    "    return ter_score\n",
    "\n",
    "# Compute BERTScore and filter by threshold\n",
    "def compute_bertscore_and_filter(test_data, sources, references, hypotheses):\n",
    "    logger.info(\"Computing BERTScore...\")\n",
    "    \n",
    "    P, R, F1 = bert_score(\n",
    "        hypotheses,\n",
    "        references,\n",
    "        lang=\"ta\", \n",
    "        model_type=\"bert-base-multilingual-cased\",\n",
    "        device=device,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    avg_f1 = F1.mean().item()\n",
    "    logger.info(f\"BERTScore F1: {avg_f1:.4f}\")\n",
    "    \n",
    "    bert_f1_scores = [f1.item() for f1 in F1]\n",
    "    \n",
    "    # Save BERTScore results\n",
    "    bertscore_df = pd.DataFrame({\n",
    "        \"telugu_sentence\": sources,\n",
    "        \"tamil_sentence\": references,\n",
    "        \"tamil_hypothesis\": hypotheses,\n",
    "        \"bertscore_f1\": bert_f1_scores\n",
    "    })\n",
    "    bertscore_df.to_csv(BERTSCORE_OUTPUT_PATH, index=False, encoding='utf-8')\n",
    "    logger.info(f\"BERTScore results saved to {BERTSCORE_OUTPUT_PATH}\")\n",
    "    \n",
    "    \n",
    "    filtered_df = bertscore_df[bertscore_df['bertscore_f1'] >= SIMILARITY_THRESHOLD]\n",
    "    filtered_df.to_csv(FILTERED_OUTPUT_PATH, index=False, encoding='utf-8')\n",
    "    logger.info(f\"Filtered results (BERTScore F1 >= {SIMILARITY_THRESHOLD}) saved to {FILTERED_OUTPUT_PATH}\")\n",
    "    print(f\"Filtered dataset size: {len(filtered_df)}\")\n",
    "    \n",
    "    return avg_f1, bert_f1_scores\n",
    "\n",
    "\n",
    "def evaluate_model():\n",
    "    test_data, sources, references, hypotheses = generate_translations(test_dataset, NUM_SAMPLES)\n",
    "    \n",
    "    bleu_score = compute_bleu(test_data, sources, references, hypotheses)\n",
    "    bertscore_f1, bert_f1_scores = compute_bertscore_and_filter(test_data, sources, references, hypotheses)\n",
    "    chrf_score = compute_chrf(test_data, sources, references, hypotheses)\n",
    "    ter_score = compute_ter(test_data, sources, references, hypotheses)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Number of samples: {len(sources)}\")\n",
    "    print(f\"Filtered samples (BERTScore F1 >= {SIMILARITY_THRESHOLD}): {len(pd.read_csv(FILTERED_OUTPUT_PATH))}\")\n",
    "    print(f\"BLEU Score: {bleu_score:.2f}\")\n",
    "    print(f\"chrF++ Score: {chrf_score:.2f}\")\n",
    "    print(f\"TER Score: {ter_score:.2f} (lower is better)\")\n",
    "    print(f\"BERTScore F1: {bertscore_f1:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    test_input = \"హలో, మీరు ఎలా ఉన్నారు?\"  # \"Hello, how are you?\" in Telugu\n",
    "    translated_text = mbart_translate_text(test_input, debug=True)\n",
    "    print(f\"\\nTest Translation:\")\n",
    "    print(f\"Source (Telugu): {test_input}\")\n",
    "    print(f\"Target (Tamil): {translated_text}\")\n",
    "    \n",
    "    return {\n",
    "        \"bleu\": bleu_score,\n",
    "        \"chrf\": chrf_score,\n",
    "        \"ter\": ter_score,\n",
    "        \"bertscore\": bertscore_f1\n",
    "    }\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
