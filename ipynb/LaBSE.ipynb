{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a83e4a-e3e3-4b28-8f85-7ee2ef1f9244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pandas in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: tqdm in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (4.66.5)\n",
      "Requirement already satisfied: langid in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (1.1.6)\n",
      "Requirement already satisfied: transformers in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: torch in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (2.3.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: filelock in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy tqdm langid transformers torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de42f0fe-020c-4fe3-be66-bd5d3a209245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 14:37:19,145 - INFO - Loading LaBSE model...\n",
      "2025-05-05 14:37:21,910 - INFO - LaBSE model loaded on cuda (GPU: NVIDIA RTX A6000)\n",
      "2025-05-05 14:37:21,946 - INFO - Starting verification using CSV file: ./Finance_Data/Finance_Data.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529968cccf7a47aa850aa3f6d1145318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing chunks: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 14:39:30,537 - INFO - Processed chunk 1 with 10000 rows. Found 3648 problematic pairs in this chunk.\n",
      "2025-05-05 14:41:02,994 - INFO - Processed chunk 2 with 7721 rows. Found 3386 problematic pairs in this chunk.\n",
      "2025-05-05 14:41:02,995 - INFO - Completed verification. Total rows processed: 17721\n",
      "2025-05-05 14:41:02,996 - INFO - Total problematic pairs found: 7034\n",
      "2025-05-05 14:41:03,079 - INFO - Saved problematic pairs to: ./Finance_Data/LaBSE/problematic_pairs.csv\n",
      "2025-05-05 14:41:03,080 - INFO - Reading full dataset to create cleaned output...\n",
      "2025-05-05 14:41:03,255 - WARNING - Output file ./Finance_Data/LaBSE/cleaned_tamil.csv may have lost Tamil or Telugu scripts.\n",
      "2025-05-05 14:41:03,288 - WARNING - Output file ./Finance_Data/LaBSE/cleaned_telugu.csv may have lost Tamil or Telugu scripts.\n",
      "2025-05-05 14:41:03,294 - INFO - Verification complete.\n",
      "2025-05-05 14:41:03,294 - INFO - Total pairs: 17721\n",
      "2025-05-05 14:41:03,294 - INFO - Problematic pairs: 7034\n",
      "2025-05-05 14:41:03,295 - INFO - Clean pairs: 10687\n",
      "2025-05-05 14:41:03,295 - INFO - Saved verification report to: ./Finance_Data/LaBSE/verification_report.txt\n",
      "2025-05-05 14:41:03,295 - INFO - Saved cleaned pairs to: ./Finance_Data/LaBSE/cleaned_pairs.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All output files are in: ./Finance_Data/LaBSE\n",
      "\n",
      "Verification Summary:\n",
      "Total pairs processed: 17721\n",
      "Problematic pairs removed: 7034\n",
      "Clean pairs retained: 10687\n",
      "\n",
      "Output files:\n",
      " - Cleaned pairs: ./Finance_Data/LaBSE/cleaned_pairs.csv\n",
      " - Cleaned Tamil: ./Finance_Data/LaBSE/cleaned_tamil.csv\n",
      " - Cleaned Telugu: ./Finance_Data/LaBSE/cleaned_telugu.csv\n",
      " - Problematic pairs: ./Finance_Data/LaBSE/problematic_pairs.csv\n",
      " - Verification report: ./Finance_Data/LaBSE/verification_report.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm  # Use notebook-specific tqdm for Jupyter\n",
    "import logging\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import Counter\n",
    "import langid\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"corpus_verification.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CorpusVerifierCSV:\n",
    "    def __init__(self, input_csv: str, output_dir: str = None,\n",
    "                 use_transformer_models: bool = True, batch_size: int = 10000,\n",
    "                 tamil_col: str = \"Tamil\", telugu_col: str = \"Telugu\"):\n",
    "        self.input_csv = input_csv\n",
    "        self.batch_size = batch_size  # Increased batch size for A6000's VRAM\n",
    "        self.use_transformer_models = use_transformer_models\n",
    "        self.tamil_col = tamil_col\n",
    "        self.telugu_col = telugu_col\n",
    "\n",
    "        # Create output dir next to input file if none is given\n",
    "        if output_dir is None:\n",
    "            base_dir = os.path.dirname(input_csv)\n",
    "            self.output_dir = os.path.join(base_dir, \"corpus_verification_output\")\n",
    "        else:\n",
    "            self.output_dir = output_dir\n",
    "\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "        self.verification_path = os.path.join(self.output_dir, 'verification_report.txt')\n",
    "        self.problematic_pairs_path = os.path.join(self.output_dir, 'problematic_pairs.csv')\n",
    "\n",
    "        if self.use_transformer_models:\n",
    "            try:\n",
    "                logger.info(\"Loading LaBSE model...\")\n",
    "                from transformers import BertModel, BertTokenizer\n",
    "                self.tokenizer = BertTokenizer.from_pretrained('setu4993/LaBSE')\n",
    "                self.model = BertModel.from_pretrained('setu4993/LaBSE')\n",
    "                self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                self.model = self.model.to(self.device)\n",
    "                logger.info(f\"LaBSE model loaded on {self.device} (GPU: {torch.cuda.get_device_name(0)})\")\n",
    "                torch.cuda.empty_cache()  # Clear GPU memory\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading LaBSE model: {e}. Disabling transformer models.\")\n",
    "                self.use_transformer_models = False\n",
    "\n",
    "        # Define script patterns for Tamil and Telugu\n",
    "        self.tamil_script_pattern = re.compile(r'[\\u0B80-\\u0BFF]')  # Tamil Unicode range\n",
    "        self.telugu_script_pattern = re.compile(r'[\\u0C00-\\u0C7F]')  # Telugu Unicode range\n",
    "\n",
    "    def _ensure_utf8(self, text: str) -> str:\n",
    "        \"\"\"Ensure text is decoded as UTF-8, replacing invalid sequences.\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        try:\n",
    "            return text.encode().decode('utf-8', errors='replace')\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to decode text: {e}. Returning empty string.\")\n",
    "            return \"\"\n",
    "\n",
    "    def _calculate_character_ratio(self, tamil_text: str, telugu_text: str) -> float:\n",
    "        tamil_len = len(self._ensure_utf8(tamil_text))\n",
    "        telugu_len = len(self._ensure_utf8(telugu_text))\n",
    "        if tamil_len == 0 or telugu_len == 0:\n",
    "            return 0.0\n",
    "        return max(tamil_len, telugu_len) / min(tamil_len, telugu_len)\n",
    "\n",
    "    def _detect_language(self, text: str) -> str:\n",
    "        try:\n",
    "            if pd.isna(text) or not text.strip():\n",
    "                return \"unknown\"\n",
    "            text = self._ensure_utf8(text)\n",
    "            lang, _ = langid.classify(text)\n",
    "            return lang\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Language detection failed: {e}\")\n",
    "            return \"unknown\"\n",
    "\n",
    "    def _script_purity(self, text: str, script_pattern) -> float:\n",
    "        if pd.isna(text) or not text.strip():\n",
    "            return 0.0\n",
    "        text = self._ensure_utf8(text)\n",
    "        script_chars = len(re.findall(script_pattern, text))\n",
    "        total_chars = len(text)\n",
    "        if total_chars == 0:\n",
    "            return 0.0\n",
    "        purity = script_chars / total_chars\n",
    "        logger.debug(f\"Script purity: {purity} for text: {text[:50]}...\")\n",
    "        return purity\n",
    "\n",
    "    def _get_sentence_embedding(self, text: str) -> np.ndarray:\n",
    "        if not self.use_transformer_models:\n",
    "            return np.zeros(1)\n",
    "        try:\n",
    "            if pd.isna(text):\n",
    "                return np.zeros(768)  # LaBSE has 768 dimensions\n",
    "            text = self._ensure_utf8(text)\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            inputs = {key: val.to(self.device) for key, val in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            embedding = outputs.pooler_output.cpu().numpy()[0]\n",
    "            embedding = embedding / np.linalg.norm(embedding)\n",
    "            return embedding\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting sentence embedding: {e}. Returning zero vector.\")\n",
    "            return np.zeros(768)\n",
    "\n",
    "    def _calculate_semantic_similarity(self, tamil_text: str, telugu_text: str) -> float:\n",
    "        if not self.use_transformer_models:\n",
    "            return 0.0\n",
    "        try:\n",
    "            tamil_embedding = self._get_sentence_embedding(tamil_text)\n",
    "            telugu_embedding = self._get_sentence_embedding(telugu_text)\n",
    "            similarity = cosine_similarity([tamil_embedding], [telugu_embedding])[0][0]\n",
    "            return float(similarity)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating semantic similarity: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def _verify_chunk(self, df_chunk: pd.DataFrame) -> List[Dict]:\n",
    "        problems = []\n",
    "        for idx, row in df_chunk.iterrows():\n",
    "            tamil_text = row[self.tamil_col]\n",
    "            telugu_text = row[self.telugu_col]\n",
    "            if pd.isna(tamil_text) or pd.isna(telugu_text):\n",
    "                problems.append({\n",
    "                    'index': idx,\n",
    "                    'tamil': tamil_text,\n",
    "                    'telugu': telugu_text,\n",
    "                    'char_ratio': 0.0,\n",
    "                    'lang_ta': 'unknown',\n",
    "                    'lang_te': 'unknown',\n",
    "                    'purity_ta': 0.0,\n",
    "                    'purity_te': 0.0,\n",
    "                    'semantic_sim': 0.0,\n",
    "                    'reason': 'missing_values'\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            tamil_text = self._ensure_utf8(tamil_text)\n",
    "            telugu_text = self._ensure_utf8(telugu_text)\n",
    "            ratio = self._calculate_character_ratio(tamil_text, telugu_text)\n",
    "            lang_ta = self._detect_language(tamil_text)\n",
    "            lang_te = self._detect_language(telugu_text)\n",
    "            purity_ta = self._script_purity(tamil_text, self.tamil_script_pattern)\n",
    "            purity_te = self._script_purity(telugu_text, self.telugu_script_pattern)\n",
    "            similarity = self._calculate_semantic_similarity(tamil_text, telugu_text) if self.use_transformer_models else 0.0\n",
    "\n",
    "            reasons = []\n",
    "            if ratio > 2.5:\n",
    "                reasons.append('length_ratio')\n",
    "            if lang_ta != 'ta':\n",
    "                reasons.append('wrong_tamil_lang')\n",
    "            if lang_te != 'te':\n",
    "                reasons.append('wrong_telugu_lang')\n",
    "            if purity_ta < 0.8:\n",
    "                reasons.append('low_tamil_script_purity')\n",
    "            if purity_te < 0.8:\n",
    "                reasons.append('low_telugu_script_purity')\n",
    "            if self.use_transformer_models and similarity < 0.7:\n",
    "                reasons.append('low_semantic_similarity')\n",
    "\n",
    "            if reasons:\n",
    "                problems.append({\n",
    "                    'index': idx,\n",
    "                    'tamil': tamil_text,\n",
    "                    'telugu': telugu_text,\n",
    "                    'char_ratio': ratio,\n",
    "                    'lang_ta': lang_ta,\n",
    "                    'lang_te': lang_te,\n",
    "                    'purity_ta': purity_ta,\n",
    "                    'purity_te': purity_te,\n",
    "                    'semantic_sim': similarity,\n",
    "                    'reason': ','.join(reasons)\n",
    "                })\n",
    "\n",
    "        return problems\n",
    "\n",
    "    def _validate_output(self, df: pd.DataFrame, file_path: str):\n",
    "        \"\"\"Validate that scripts are preserved in the output file.\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            tamil_check = self.tamil_script_pattern.search(content)\n",
    "            telugu_check = self.telugu_script_pattern.search(content)\n",
    "            if not tamil_check or not telugu_check:\n",
    "                logger.warning(f\"Output file {file_path} may have lost Tamil or Telugu scripts.\")\n",
    "\n",
    "    def run_verification(self):\n",
    "        logger.info(f\"Starting verification using CSV file: {self.input_csv}\")\n",
    "\n",
    "        # Read CSV file in chunks - explicitly specify encoding\n",
    "        all_problems = []\n",
    "        chunk_num = 0\n",
    "        total_rows = 0\n",
    "\n",
    "        for df_chunk in tqdm(pd.read_csv(self.input_csv, chunksize=self.batch_size, encoding='utf-8'),\n",
    "                             desc=\"Processing chunks\"):\n",
    "            chunk_num += 1\n",
    "            total_rows += len(df_chunk)\n",
    "\n",
    "            # Check if required columns exist\n",
    "            if self.tamil_col not in df_chunk.columns or self.telugu_col not in df_chunk.columns:\n",
    "                logger.error(f\"Required columns '{self.tamil_col}' and/or '{self.telugu_col}' not found in CSV\")\n",
    "                raise ValueError(f\"Required columns not found. Available columns: {df_chunk.columns.tolist()}\")\n",
    "\n",
    "            # Process this chunk\n",
    "            problems = self._verify_chunk(df_chunk)\n",
    "            all_problems.extend(problems)\n",
    "\n",
    "            logger.info(f\"Processed chunk {chunk_num} with {len(df_chunk)} rows. \"\n",
    "                        f\"Found {len(problems)} problematic pairs in this chunk.\")\n",
    "\n",
    "        logger.info(f\"Completed verification. Total rows processed: {total_rows}\")\n",
    "        logger.info(f\"Total problematic pairs found: {len(all_problems)}\")\n",
    "\n",
    "        # Create DataFrame with problematic pairs\n",
    "        if all_problems:\n",
    "            problem_df = pd.DataFrame(all_problems)\n",
    "            problem_df.to_csv(self.problematic_pairs_path, index=False, encoding='utf-8')\n",
    "            self._validate_output(problem_df, self.problematic_pairs_path)\n",
    "            logger.info(f\"Saved problematic pairs to: {self.problematic_pairs_path}\")\n",
    "        else:\n",
    "            logger.info(\"No problematic pairs found.\")\n",
    "            pd.DataFrame(columns=['index', 'tamil', 'telugu', 'char_ratio', 'lang_ta', 'lang_te',\n",
    "                                 'purity_ta', 'purity_te', 'semantic_sim', 'reason']\n",
    "                        ).to_csv(self.problematic_pairs_path, index=False, encoding='utf-8')\n",
    "            self._validate_output(pd.DataFrame(), self.problematic_pairs_path)\n",
    "\n",
    "        # Read complete dataset with UTF-8 encoding\n",
    "        logger.info(\"Reading full dataset to create cleaned output...\")\n",
    "        full_df = pd.read_csv(self.input_csv, encoding='utf-8')\n",
    "\n",
    "        # Get indices of problematic rows\n",
    "        problem_indices = set(p['index'] for p in all_problems)\n",
    "\n",
    "        # Create cleaned dataframe\n",
    "        clean_df = full_df[~full_df.index.isin(problem_indices)].copy()\n",
    "\n",
    "        # Save cleaned files with UTF-8 encoding\n",
    "        cleaned_csv_path = os.path.join(self.output_dir, 'cleaned_pairs.csv')\n",
    "        cleaned_tamil_path = os.path.join(self.output_dir, 'cleaned_tamil.csv')\n",
    "        cleaned_telugu_path = os.path.join(self.output_dir, 'cleaned_telugu.csv')\n",
    "\n",
    "        clean_df.to_csv(cleaned_csv_path, index=False, encoding='utf-8')\n",
    "        self._validate_output(clean_df, cleaned_csv_path)\n",
    "        clean_df[[self.tamil_col]].to_csv(cleaned_tamil_path, index=False, encoding='utf-8')\n",
    "        self._validate_output(clean_df[[self.tamil_col]], cleaned_tamil_path)\n",
    "        clean_df[[self.telugu_col]].to_csv(cleaned_telugu_path, index=False, encoding='utf-8')\n",
    "        self._validate_output(clean_df[[self.telugu_col]], cleaned_telugu_path)\n",
    "\n",
    "        # Counts for summary\n",
    "        total_pairs = len(full_df)\n",
    "        problematic_pairs = len(all_problems)\n",
    "        clean_pairs = total_pairs - problematic_pairs\n",
    "\n",
    "        # Write verification summary\n",
    "        with open(self.verification_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Total pairs: {total_pairs}\\n\")\n",
    "            f.write(f\"Problematic pairs: {problematic_pairs}\\n\")\n",
    "            f.write(f\"Clean pairs: {clean_pairs}\\n\")\n",
    "            f.write(f\"\\nProblematic pairs file: {self.problematic_pairs_path}\\n\")\n",
    "            f.write(f\"Cleaned full pairs file: {cleaned_csv_path}\\n\")\n",
    "            f.write(f\"Cleaned Tamil file: {cleaned_tamil_path}\\n\")\n",
    "            f.write(f\"Cleaned Telugu file: {cleaned_telugu_path}\\n\")\n",
    "\n",
    "            if all_problems:\n",
    "                reason_counts = Counter()\n",
    "                for p in all_problems:\n",
    "                    for reason in p['reason'].split(','):\n",
    "                        reason_counts[reason] += 1\n",
    "\n",
    "                f.write(\"\\nBreakdown of problematic pairs by reason:\\n\")\n",
    "                for reason, count in reason_counts.most_common():\n",
    "                    f.write(f\"  - {reason}: {count}\\n\")\n",
    "\n",
    "        logger.info(f\"Verification complete.\")\n",
    "        logger.info(f\"Total pairs: {total_pairs}\")\n",
    "        logger.info(f\"Problematic pairs: {problematic_pairs}\")\n",
    "        logger.info(f\"Clean pairs: {clean_pairs}\")\n",
    "        logger.info(f\"Saved verification report to: {self.verification_path}\")\n",
    "        logger.info(f\"Saved cleaned pairs to: {cleaned_csv_path}\")\n",
    "\n",
    "        print(f\"\\nAll output files are in: {self.output_dir}\")\n",
    "        print(f\"\\nVerification Summary:\")\n",
    "        print(f\"Total pairs processed: {total_pairs}\")\n",
    "        print(f\"Problematic pairs removed: {problematic_pairs}\")\n",
    "        print(f\"Clean pairs retained: {clean_pairs}\")\n",
    "        print(\"\\nOutput files:\")\n",
    "        print(f\" - Cleaned pairs: {cleaned_csv_path}\")\n",
    "        print(f\" - Cleaned Tamil: {cleaned_tamil_path}\")\n",
    "        print(f\" - Cleaned Telugu: {cleaned_telugu_path}\")\n",
    "        print(f\" - Problematic pairs: {self.problematic_pairs_path}\")\n",
    "        print(f\" - Verification report: {self.verification_path}\")\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual CSV file path (local or server path)\n",
    "    input_csv = \"./Finance_Data/Finance_Data.csv\"  # Adjust to your local path\n",
    "\n",
    "    verifier = CorpusVerifierCSV(\n",
    "        input_csv=input_csv,\n",
    "        output_dir=\"./Finance_Data/LaBSE\",  # Adjust to your desired output path\n",
    "        use_transformer_models=True,  # Leverage A6000 GPU\n",
    "        batch_size=10000,  # Optimized for A6000\n",
    "        tamil_col=\"Tamil\",\n",
    "        telugu_col=\"Telugu\"\n",
    "    )\n",
    "\n",
    "    verifier.run_verification()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
