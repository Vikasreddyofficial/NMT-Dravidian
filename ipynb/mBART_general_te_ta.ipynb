{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f22eedea",
   "metadata": {},
   "source": [
    "#Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2840b813-2dc1-4939-aba8-dbc47e77b5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets sentencepiece sacrebleu accelerate gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6e828b-44f7-4fc1-8a6d-d9737000d3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (4.41.2)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tokenizers, transformers\n",
      "\u001b[2K  Attempting uninstall: tokenizers\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[2K    Found existing installation: tokenizers 0.19.1\n",
      "\u001b[2K    Uninstalling tokenizers-0.19.1:\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.19.1\n",
      "\u001b[2K  Attempting uninstall: transformers\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[2K    Found existing installation: transformers 4.41.2\n",
      "\u001b[2K    Uninstalling transformers-4.41.2:\n",
      "\u001b[2K      Successfully uninstalled transformers-4.41.25;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [transformers]0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[1A\u001b[2K\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed tokenizers-0.21.1 transformers-4.51.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.0.1 torchvision==0.15.2 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16baf98e-9e13-4d05-abfc-c8c4b3b5e28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 17:03:18.040713: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-05 17:03:18.053125: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746444798.067569  100316 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746444798.072122  100316 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746444798.082851  100316 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746444798.082868  100316 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746444798.082870  100316 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746444798.082871  100316 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-05 17:03:18.086931: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "DataFrame shape: (547567, 2)\n",
      "Sample data:\n",
      "                                                Tamil  \\\n",
      "0     அவள் பெயர் கூட அவளுக்கு ஒன்றும் நினைவில் இல்லை   \n",
      "1  சமைப்பது வேகமானது இதன் விளைவாக ஊட்டச்சத்துக்கள...   \n",
      "2  நாம் ஏற்கனவே செய்வதை ரசிப்பதைக் கண்டுபிடிப்பதற...   \n",
      "3  இது ஒரு மேனுவல் அல்லது ஆட்டோமேட்டிக் கியர்பாக்...   \n",
      "4                           இதுவும் நல்ல முயற்சிதான்   \n",
      "\n",
      "                                              Telugu  \n",
      "0               కనీసం ఆమె పేరు కూడా ఆయనకు గుర్తులేదు  \n",
      "1  వంట వేగంగా ఉంటుంది తద్వారా పోషకాలు మరియు విటమి...  \n",
      "2  మనం ఇప్పటికే ఆనందించేదాన్ని గుర్తించడానికి బదు...  \n",
      "3  ఇది మాన్యువల్ లేదా ఆటోమేటిక్ గేర్బాక్స్తో పెట్...  \n",
      "4                ఇది కూడా మంచి ఉపయోగ కరమైన ప్రయత్నమే  \n",
      "Train dataset size: 492810\n",
      "Test dataset size: 54757\n",
      "mBART - Initial tokenizer vocab size: 250054\n",
      "mBART - Initial model output vocab size: 250054\n",
      "mBART - Vocab sizes match, no adjustment needed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90593c6ffa64683819dc81ea925db41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/492810 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79ab9ec2e1e4693b2c33ce1bf3bda38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/54757 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mBART - Tokenized train sample: {'input_ids': [250045, 60078, 1296, 483, 6, 136571, 27013, 14206, 4276, 103646, 95432, 8197, 55763, 5271, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [250044, 2690, 3770, 63277, 235753, 8182, 15453, 483, 55963, 86322, 78611, 8285, 6149, 80334, 8182, 131846, 8182, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n",
      "Training mBART...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_100316/1919149387.py:90: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  mbart_trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='77005' max='77005' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [77005/77005 9:19:06, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.089700</td>\n",
       "      <td>1.052916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.893200</td>\n",
       "      <td>0.953157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.755900</td>\n",
       "      <td>0.919803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.633800</td>\n",
       "      <td>0.912127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.551400</td>\n",
       "      <td>0.923989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mBART - Saved tokenizer vocab size: 250054\n",
      "mBART - Saved model output vocab size: 250054\n",
      "Tokenized Input IDs: [[250045, 24147, 1296, 4, 22735, 24722, 91064, 32, 2]]\n",
      "Raw Output IDs: [2, 250044, 39507, 66705, 4, 19238, 29947, 128251, 37961, 32, 2]\n",
      "Decoded with special tokens: </s>ta_IN ஹலோ, நீ எப்படி இருக்கிறாய்?</s>\n",
      "mBART Translation: ஹலோ, நீ எப்படி இருக்கிறாய்?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import MBart50TokenizerFast, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('./merged_output.csv')\n",
    "df1 = df.copy()  # Create a copy to avoid any warnings\n",
    "df1.drop(['Unnamed: 0.1', 'Unnamed: 0'], axis=1, inplace=True, errors='ignore')  # Handle missing columns gracefully\n",
    "\n",
    "\n",
    "print(\"DataFrame shape:\", df1.shape)\n",
    "print(\"Sample data:\\n\", df1.head(5))  # Show 5 rows\n",
    "\n",
    "\n",
    "df1 = df1.rename(columns={\"Tamil\": \"ta\", \"Telugu\": \"te\"})\n",
    "\n",
    "\n",
    "dataset = Dataset.from_pandas(df1)\n",
    "split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "split_dataset = DatasetDict({\"train\": split_dataset[\"train\"], \"test\": split_dataset[\"test\"]})\n",
    "print(\"Train dataset size:\", len(split_dataset[\"train\"]))\n",
    "print(\"Test dataset size:\", len(split_dataset[\"test\"]))\n",
    "\n",
    "# Load mBART model and tokenizer\n",
    "MBART_MODEL_NAME = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "mbart_tokenizer = MBart50TokenizerFast.from_pretrained(MBART_MODEL_NAME, src_lang=\"te_IN\", tgt_lang=\"ta_IN\")\n",
    "mbart_model = AutoModelForSeq2SeqLM.from_pretrained(MBART_MODEL_NAME).to(device)\n",
    "\n",
    "\n",
    "mbart_vocab_size_tokenizer = len(mbart_tokenizer)\n",
    "mbart_vocab_size_model = mbart_model.get_output_embeddings().weight.size(0)\n",
    "print(\"mBART - Initial tokenizer vocab size:\", mbart_vocab_size_tokenizer)\n",
    "print(\"mBART - Initial model output vocab size:\", mbart_vocab_size_model)\n",
    "\n",
    "\n",
    "if mbart_vocab_size_tokenizer != mbart_vocab_size_model:\n",
    "    print(f\"Warning: mBART vocab size mismatch (Tokenizer: {mbart_vocab_size_tokenizer}, Model: {mbart_vocab_size_model}). Adjusting model embeddings.\")\n",
    "    mbart_model.resize_token_embeddings(mbart_vocab_size_tokenizer)\n",
    "    print(\"Post-resize model vocab size:\", mbart_model.get_output_embeddings().weight.size(0))\n",
    "else:\n",
    "    print(\"mBART - Vocab sizes match, no adjustment needed.\")\n",
    "\n",
    "# Preprocessing function\n",
    "def mbart_preprocess_function(examples):\n",
    "    inputs = [te_text for te_text in examples[\"te\"]]  # Telugu as input\n",
    "    targets = [ta_text for ta_text in examples[\"ta\"]]  # Tamil as target\n",
    "    model_inputs = mbart_tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    with mbart_tokenizer.as_target_tokenizer():\n",
    "        labels = mbart_tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\").input_ids\n",
    "    labels = [[-100 if token == mbart_tokenizer.pad_token_id else token for token in seq] for seq in labels]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "mbart_tokenized_datasets = split_dataset.map(\n",
    "    mbart_preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    remove_columns=[\"ta\", \"te\"]\n",
    ")\n",
    "print(\"mBART - Tokenized train sample:\", mbart_tokenized_datasets[\"train\"][0])\n",
    "\n",
    "# Training arguments\n",
    "mbart_training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mbart_finetuned_te_to_ta\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=5,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    logging_steps=500,\n",
    "    save_steps=5000\n",
    ")\n",
    "\n",
    "# Data collator and trainer\n",
    "mbart_data_collator = DataCollatorForSeq2Seq(mbart_tokenizer, model=mbart_model)\n",
    "mbart_trainer = Seq2SeqTrainer(\n",
    "    model=mbart_model,\n",
    "    args=mbart_training_args,\n",
    "    train_dataset=mbart_tokenized_datasets[\"train\"],\n",
    "    eval_dataset=mbart_tokenized_datasets[\"test\"],\n",
    "    tokenizer=mbart_tokenizer,\n",
    "    data_collator=mbart_data_collator\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"Training mBART...\")\n",
    "mbart_trainer.train()\n",
    "\n",
    "# Save\n",
    "mbart_trainer.save_model(\"./mbart_finetuned_te_to_ta\")\n",
    "mbart_tokenizer.save_pretrained(\"./mbart_finetuned_te_to_ta\")\n",
    "\n",
    "# Verify saved model\n",
    "mbart_saved_model = AutoModelForSeq2SeqLM.from_pretrained(\"./mbart_finetuned_te_to_ta\").to(device)\n",
    "mbart_saved_tokenizer = MBart50TokenizerFast.from_pretrained(\"./mbart_finetuned_te_to_ta\", src_lang=\"te_IN\", tgt_lang=\"ta_IN\")\n",
    "print(\"mBART - Saved tokenizer vocab size:\", len(mbart_saved_tokenizer))\n",
    "print(\"mBART - Saved model output vocab size:\", mbart_saved_model.get_output_embeddings().weight.size(0))\n",
    "\n",
    "# Test translation with debugging\n",
    "def mbart_translate_text(input_text, debug=False):\n",
    "    inputs = mbart_saved_tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True, padding=True).to(device)\n",
    "    if debug:\n",
    "        print(\"Tokenized Input IDs:\", inputs[\"input_ids\"].tolist())\n",
    "    outputs = mbart_saved_model.generate(\n",
    "        **inputs,\n",
    "        max_length=256,\n",
    "        min_length=10,\n",
    "        num_beams=5,\n",
    "        early_stopping=False,\n",
    "        length_penalty=1.0,\n",
    "        no_repeat_ngram_size=2,\n",
    "        forced_bos_token_id=mbart_saved_tokenizer.lang_code_to_id[\"ta_IN\"]\n",
    "    )\n",
    "    if debug:\n",
    "        print(\"Raw Output IDs:\", outputs[0].tolist())\n",
    "        print(\"Decoded with special tokens:\", mbart_saved_tokenizer.decode(outputs[0], skip_special_tokens=False))\n",
    "    decoded_output = mbart_saved_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return decoded_output.strip()\n",
    "\n",
    "# Test\n",
    "input_text = \"హలో, మీరు ఎలా ఉన్నారు?\"  # \"Hello, how are you?\" in Telugu\n",
    "translated_text = mbart_translate_text(input_text, debug=True)\n",
    "print(\"mBART Translation:\", translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eebf198",
   "metadata": {},
   "source": [
    "#Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7840f7-93fb-4ca0-8595-aabf1c35a216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 15:43:58.014780: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-06 15:43:58.026548: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746526438.040229  306520 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746526438.044411  306520 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746526438.054874  306520 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746526438.054889  306520 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746526438.054891  306520 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746526438.054893  306520 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-06 15:43:58.058736: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Telugu to Tamil Translator</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Telugu text below to translate to Tamil (type 'exit' to stop):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telugu Input:  నేను పుస్తకం చదువుతున్నాను.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Telugu:</b> నేను పుస్తకం చదువుతున్నాను.<br><b>Tamil Translation:</b> நான் புத்தகத்தைப் படித்துக் கொண்டிருக்கிறேன்.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telugu Input:  నీవు రేపు రాగలవా?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Telugu:</b> నీవు రేపు రాగలవా?<br><b>Tamil Translation:</b> நாளைக்கு நீ வருவாயா, வியாழக்கிழமையா?</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telugu Input:  ఏఐ వినియోగం రోజురోజుకూ విపరీతంగా పెరుగుతోంది.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Telugu:</b> ఏఐ వినియోగం రోజురోజుకూ విపరీతంగా పెరుగుతోంది.<br><b>Tamil Translation:</b> ஏ. ஐ. யின் பயன்பாடு நாளுக்கு நாள் அதிகரித்து வருகிறது.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telugu Input:  ఈ తరుణంలో ఓపెన్‌ఏఐ (OpenAI) సంస్థ చాట్‌జీపీటీ (ChatGPT) చాట్‌బాట్‌ వంటివి సంచలనాలు సృష్టిస్తుంటే.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Telugu:</b> ఈ తరుణంలో ఓపెన్‌ఏఐ (OpenAI) సంస్థ చాట్‌జీపీటీ (ChatGPT) చాట్‌బాట్‌ వంటివి సంచలనాలు సృష్టిస్తుంటే.<br><b>Tamil Translation:</b> ஓப்பன் ஏ. ஐ. , சேட் ஜிபிட்டி (ChatGPT) போன்ற நிறுவனங்கள் இந்த சூழலில் பரபரப்பை ஏற்படுத்தி வருகின்றன.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telugu Input:  చాట్ జీపీటీకి యూజర్ బేస్ పెరిగిన నేపథ్యంలో అనేక ఏఐలు పోటీగా మార్కెట్లోకి వస్తున్నాయి.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Telugu:</b> చాట్ జీపీటీకి యూజర్ బేస్ పెరిగిన నేపథ్యంలో అనేక ఏఐలు పోటీగా మార్కెట్లోకి వస్తున్నాయి.<br><b>Tamil Translation:</b> அரட்டையின் ஜி. பி. டி. க்கு இணைய பயனர்களின் எண்ணிக்கை அதிகரித்து வருவதால் ஏராளமான ஏ. ஐ. நிறுவனங்கள் சந்தையில் போட்டியிடுகின்றன.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telugu Input:  \tTelugu 3\t2016 సంవత్సరంలో 62.7 బిలియన్‌ డాలర్లు, 2017 65.3 బిలియన్‌ డాలర్లుగా ఉంది.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Telugu:</b> Telugu 3\t2016 సంవత్సరంలో 62.7 బిలియన్‌ డాలర్లు, 2017 65.3 బిలియన్‌ డాలర్లుగా ఉంది.<br><b>Tamil Translation:</b> 2016 ஆம் ஆண்டில் தமிழ் மொழி 62.7 பில்லியன் டாலராகவும், 2017 இல் தெலுங்கு மொழி 65.3 பில்லியனாகவும் இருந்தது.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telugu Input:  2016 సంవత్సరంలో 62.7 బిలియన్‌ డాలర్లు, 2017 65.3 బిలియన్‌ డాలర్లుగా ఉంది.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><b>Telugu:</b> 2016 సంవత్సరంలో 62.7 బిలియన్‌ డాలర్లు, 2017 65.3 బిలియన్‌ డాలర్లుగా ఉంది.<br><b>Tamil Translation:</b> 2016 ஆம் ஆண்டில் இது 62.7 பில்லியன் டாலராகவும், 2017 இல் இது 65.3 பில்லியனாகவும் இருந்தது.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telugu Input:  exit\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style='color: green;'>Exiting translator...</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MBart50TokenizerFast, AutoModelForSeq2SeqLM\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "MODEL_PATH = \"./mbart_finetuned_te_to_ta\"  # Ensure this model is fine-tuned for Telugu-to-Tamil\n",
    "mbart_saved_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH).to(device)\n",
    "mbart_saved_tokenizer = MBart50TokenizerFast.from_pretrained(MODEL_PATH, src_lang=\"te_IN\", tgt_lang=\"ta_IN\")\n",
    "\n",
    "\n",
    "def mbart_translate_text(input_text, debug=False):\n",
    "    inputs = mbart_saved_tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True, padding=True).to(device)\n",
    "    if debug:\n",
    "        print(\"Tokenized Input IDs:\", inputs[\"input_ids\"].tolist())\n",
    "    outputs = mbart_saved_model.generate(\n",
    "        **inputs,\n",
    "        max_length=256,\n",
    "        min_length=10,\n",
    "        num_beams=5,\n",
    "        early_stopping=False,\n",
    "        length_penalty=1.0,\n",
    "        no_repeat_ngram_size=2,\n",
    "        forced_bos_token_id=mbart_saved_tokenizer.lang_code_to_id[\"ta_IN\"]  # Tamil\n",
    "    )\n",
    "    if debug:\n",
    "        print(\"Raw Output IDs:\", outputs[0].tolist())\n",
    "        print(\"Decoded with special tokens:\", mbart_saved_tokenizer.decode(outputs[0], skip_special_tokens=False))\n",
    "    decoded_output = mbart_saved_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return decoded_output.strip()\n",
    "\n",
    "\n",
    "def translate_interactively():\n",
    "    display(HTML(\"<h3>Telugu to Tamil Translator</h3>\"))\n",
    "    print(\"Enter Telugu text below to translate to Tamil (type 'exit' to stop):\")\n",
    "    \n",
    "    while True:\n",
    "      \n",
    "        user_input = input(\"Telugu Input: \").strip()\n",
    "        \n",
    "        #  exit condition\n",
    "        if user_input.lower() == \"exit\":\n",
    "            display(HTML(\"<p style='color: green;'>Exiting translator...</p>\"))\n",
    "            break\n",
    "        \n",
    "        if not user_input:\n",
    "            display(HTML(\"<p style='color: red;'>Please enter some text.</p>\"))\n",
    "            continue\n",
    "        \n",
    "     \n",
    "        try:\n",
    "            translated_text = mbart_translate_text(user_input, debug=False)  # Set debug=True for detailed output\n",
    "            display(HTML(f\"<p><b>Telugu:</b> {user_input}<br><b>Tamil Translation:</b> {translated_text}</p>\"))\n",
    "        except Exception as e:\n",
    "            display(HTML(f\"<p style='color: red;'>Error during translation: {e}</p>\"))\n",
    "\n",
    "\n",
    "translate_interactively()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a43baa",
   "metadata": {},
   "source": [
    "#Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be00ef2-58db-42ce-9337-0de86313f575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model configuration...\n",
      "Loading model and tokenizer...\n",
      "Loading dataset...\n",
      "Generating translations for 1260 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: ['English', 'Telugu', 'Tamil']\n",
      "Dataset shape: (1263, 3)\n",
      "First few rows:\n",
      "                                             English  \\\n",
      "0  The two leaders also discussed global developm...   \n",
      "1  On the occasion of Ambedkar Jayanti today, Pri...   \n",
      "2  He said the Government is working with a diffe...   \n",
      "3  He said the aim is to complete this task by 2022.   \n",
      "4  He said these Health and Wellness Centres woul...   \n",
      "\n",
      "                                              Telugu  \\\n",
      "0  ఉభ‌య నేత‌లు ప్ర‌పంచ అభివృద్ధి సంబంధిత ఆర్థిక స...   \n",
      "1  నేడు ఆంబేడ్ కర్ జయంతి సందర్భంగా, ప్రధాన మంత్రి...   \n",
      "2  ఈ 115 జిల్లాల విషయంలో ప్రభుత్వం ఒక వ్యత్యాసభరి...   \n",
      "3  ఈ కార్యభారాన్ని 2022 కల్లా పూర్తి చేయాలన్నదే ల...   \n",
      "4  ఈ హెల్త్ అండ్ వెల్ నెస్ సెంటర్ లు పేదలకు ఒక కు...   \n",
      "\n",
      "                                               Tamil  \n",
      "0  உலகளாவிய மேம்பாட்டுக்கான நிதி உள்ளிட்டவை குறித...  \n",
      "1  அம்பேத்கர் பிறந்த தினமான இன்று, மத்திய அரசின் ...  \n",
      "2  இந்த 115 மாவட்டங்கள் மீதும் மாறுபட்ட கண்ணோட்டத...  \n",
      "3  இந்தப் பணியை 2022-ம் ஆண்டுக்குள் நிறைவேற்றுவதே...  \n",
      "4  இந்தச் சுகாதார மற்றும் ஆரோக்கிய மையங்கள், ஏழைம...  \n",
      "Using Telugu column: Telugu\n",
      "Using Tamil column: Tamil\n",
      "Test dataset size: 1263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating translations:   0%|                         | 0/1260 [00:00<?, ?it/s]/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:554: UserWarning: `num_beams` is set to None - defaulting to 1.\n",
      "  warnings.warn(\"`num_beams` is set to None - defaulting to 1.\", UserWarning)\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/utils.py:1283: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Generating translations: 100%|██████████████| 1260/1260 [10:51<00:00,  1.93it/s]\n",
      "Computing BLEU score...\n",
      "BLEU Score: 39.13\n",
      "BLEU results saved to ./mBART_Reverse_RESULTS/bleu_evaluation_results.csv\n",
      "Computing BERTScore...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06fdf2ba0de64f618692fae95805d31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438aead6323e4a718509d3dccae02708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BERTScore F1: 0.8496\n",
      "BERTScore results saved to ./mBART_Reverse_RESULTS/bertscore_evaluation_results.csv\n",
      "Computing chrF++ score...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 4.93 seconds, 255.68 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chrF++ Score: 42.48\n",
      "chrF++ results saved to ./mBART_Reverse_RESULTS/chrf_evaluation_results.csv\n",
      "Computing TER score...\n",
      "TER Score: 81.08\n",
      "TER results saved to ./mBART_Reverse_RESULTS/ter_evaluation_results.csv\n",
      "Computing COMET score...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d603259d0127446fba5b053d84fc2af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Encoder model frozen.\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "Running COMET evaluation...\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|████████████████| 158/158 [00:12<00:00, 12.40it/s]\n",
      "COMET Score: 0.9203\n",
      "COMET results saved to ./mBART_Reverse_RESULTS/comet_evaluation_results.csv\n",
      "Tokenized Input IDs: [[250045, 47409, 49833, 4, 22735, 24722, 91064, 32, 2]]\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:554: UserWarning: `num_beams` is set to None - defaulting to 1.\n",
      "  warnings.warn(\"`num_beams` is set to None - defaulting to 1.\", UserWarning)\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EVALUATION SUMMARY\n",
      "==================================================\n",
      "Number of samples: 1260\n",
      "BLEU Score: 39.13\n",
      "chrF++ Score: 42.48\n",
      "TER Score: 81.08 (lower is better)\n",
      "BERTScore F1: 0.8496\n",
      "COMET Score: 0.9203\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Raw Output IDs: [2, 250044, 39507, 37961, 4, 19238, 29947, 128251, 37961, 32, 2]\n",
      "Decoded with special tokens: </s>ta_IN ஹாய், நீ எப்படி இருக்கிறாய்?</s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Translation:\n",
      "Source (Telugu): హాయ్, మీరు ఎలా ఉన్నారు?\n",
      "Target (Tamil): ஹாய், நீ எப்படி இருக்கிறாய்?\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration, AutoConfig, AutoModelForSeq2SeqLM\n",
    "from sacrebleu import corpus_bleu, corpus_chrf, corpus_ter\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "\n",
    "try:\n",
    "    from comet import download_model, load_from_checkpoint\n",
    "    comet_available = True\n",
    "except ImportError:\n",
    "    comet_available = False\n",
    "    print(\"COMET not available. Will skip COMET evaluation.\")\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "MODEL_PATH = \"./mbart_finetuned_te_to_ta\"  \n",
    "DATASET_PATH = \"./testing_pairs.csv\" \n",
    "NUM_SAMPLES = 1260  \n",
    "\n",
    "# Output paths\n",
    "BLEU_OUTPUT_PATH = \"./mBART_Reverse_RESULTS/bleu_evaluation_results.csv\"\n",
    "BERTSCORE_OUTPUT_PATH = \"./mBART_Reverse_RESULTS/bertscore_evaluation_results.csv\"\n",
    "COMET_OUTPUT_PATH = \"./mBART_Reverse_RESULTS/comet_evaluation_results.csv\"\n",
    "CHRF_OUTPUT_PATH = \"./mBART_Reverse_RESULTS/chrf_evaluation_results.csv\"\n",
    "TER_OUTPUT_PATH = \"./mBART_Reverse_RESULTS/ter_evaluation_results.csv\"\n",
    "\n",
    "# Load the model configuration first\n",
    "logger.info(\"Loading model configuration...\")\n",
    "config = AutoConfig.from_pretrained(MODEL_PATH)\n",
    "if hasattr(config, 'generation_config'):\n",
    "    if config.generation_config.early_stopping is None:\n",
    "        config.generation_config.early_stopping = True\n",
    "else:\n",
    "    config.early_stopping = True\n",
    "\n",
    "\n",
    "logger.info(\"Loading model and tokenizer...\")\n",
    "try:\n",
    "    mbart_saved_model = MBartForConditionalGeneration.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        config=config\n",
    "    ).to(device)\n",
    "except:\n",
    "    mbart_saved_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        config=config\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "if hasattr(mbart_saved_model, 'generation_config'):\n",
    "    mbart_saved_model.generation_config.early_stopping = True\n",
    "\n",
    "mbart_saved_tokenizer = MBart50TokenizerFast.from_pretrained(MODEL_PATH, src_lang=\"te_IN\", tgt_lang=\"ta_IN\")\n",
    "\n",
    "# Load the test dataset\n",
    "logger.info(\"Loading dataset...\")\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "print(f\"Dataset columns: {df.columns.tolist()}\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"First few rows:\\n{df.head()}\")\n",
    "\n",
    "\n",
    "telugu_col = None\n",
    "tamil_col = None\n",
    "\n",
    "\n",
    "telugu_patterns = ['telugu_sentence', 'telugu', 'source', 'src', 'Telugu', 'telugu_text']\n",
    "tamil_patterns = ['tamil_sentence', 'tamil', 'target', 'tgt', 'Tamil', 'tamil_text']\n",
    "\n",
    "for col in df.columns:\n",
    "    if any(pattern.lower() in col.lower() for pattern in telugu_patterns):\n",
    "        telugu_col = col\n",
    "    if any(pattern.lower() in col.lower() for pattern in tamil_patterns):\n",
    "        tamil_col = col\n",
    "\n",
    "if telugu_col is None or tamil_col is None:\n",
    "    raise ValueError(f\"Could not identify Telugu and Tamil columns. Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "print(f\"Using Telugu column: {telugu_col}\")\n",
    "print(f\"Using Tamil column: {tamil_col}\")\n",
    "\n",
    "\n",
    "df = df[[telugu_col, tamil_col]].dropna()\n",
    "\n",
    "df = df.rename(columns={telugu_col: 'telugu_sentence', tamil_col: 'tamil_sentence'})\n",
    "test_dataset = Dataset.from_pandas(df)\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "def indic_tokenize_text(text):\n",
    "    if not text or pd.isna(text):\n",
    "        return \"\"\n",
    "    return ' '.join(indic_tokenize.trivial_tokenize(text, lang='ta'))\n",
    "\n",
    "\n",
    "def mbart_translate_text(input_text, debug=False):\n",
    "    inputs = mbart_saved_tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True, padding=True).to(device)\n",
    "    if debug:\n",
    "        logger.info(f\"Tokenized Input IDs: {inputs['input_ids'].tolist()}\")\n",
    "    outputs = mbart_saved_model.generate(\n",
    "        **inputs,\n",
    "        max_length=256,\n",
    "        min_length=10,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        length_penalty=1.2,\n",
    "        no_repeat_ngram_size=3,\n",
    "        forced_bos_token_id=mbart_saved_tokenizer.lang_code_to_id[\"ta_IN\"]\n",
    "    )\n",
    "    if debug:\n",
    "        logger.info(f\"Raw Output IDs: {outputs[0].tolist()}\")\n",
    "        logger.info(f\"Decoded with special tokens: {mbart_saved_tokenizer.decode(outputs[0], skip_special_tokens=False)}\")\n",
    "    decoded_output = mbart_saved_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return decoded_output.strip()\n",
    "\n",
    "\n",
    "def generate_translations(dataset, num_samples=NUM_SAMPLES):\n",
    "    sources = []\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    test_data = dataset.select(range(min(num_samples, len(dataset))))\n",
    "    logger.info(f\"Generating translations for {len(test_data)} samples\")\n",
    "    \n",
    "    for example in tqdm(test_data, desc=\"Generating translations\"):\n",
    "        input_text = example[\"telugu_sentence\"]\n",
    "        reference = example[\"tamil_sentence\"]\n",
    "        \n",
    "        try:\n",
    "            hypothesis = mbart_translate_text(input_text, debug=False)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error translating '{input_text}': {e}\")\n",
    "            hypothesis = \"\"\n",
    "            \n",
    "        sources.append(input_text)\n",
    "        references.append(reference)\n",
    "        hypotheses.append(hypothesis)\n",
    "    \n",
    "    return test_data, sources, references, hypotheses\n",
    "\n",
    "# Compute BLEU score\n",
    "def compute_bleu(test_data, sources, references, hypotheses):\n",
    "    logger.info(\"Computing BLEU score...\")\n",
    "    \n",
    "    tokenized_hypotheses = [indic_tokenize_text(hyp) for hyp in hypotheses]\n",
    "    tokenized_references = [[indic_tokenize_text(ref)] for ref in references]\n",
    "    \n",
    "    bleu = corpus_bleu(tokenized_hypotheses, tokenized_references, tokenize='none')\n",
    "    bleu_score = bleu.score\n",
    "    logger.info(f\"BLEU Score: {bleu_score:.2f}\")\n",
    "    \n",
    "    results_df = pd.DataFrame({\n",
    "        \"telugu_sentence\": sources,\n",
    "        \"tamil_sentence\": references,\n",
    "        \"tamil_hypothesis\": hypotheses,\n",
    "        \"bleu_score\": [bleu_score] * len(sources)\n",
    "    })\n",
    "    results_df.to_csv(BLEU_OUTPUT_PATH, index=False)\n",
    "    logger.info(f\"BLEU results saved to {BLEU_OUTPUT_PATH}\")\n",
    "    \n",
    "    return bleu_score\n",
    "    \n",
    "# Compute chrF++ score\n",
    "def compute_chrf(test_data, sources, references, hypotheses):\n",
    "    logger.info(\"Computing chrF++ score...\")\n",
    "    \n",
    "    refs_list = [[ref] for ref in references]\n",
    "    \n",
    "    chrf = corpus_chrf(hypotheses, refs_list, char_order=6, word_order=2, beta=2)\n",
    "    chrf_score = chrf.score\n",
    "    logger.info(f\"chrF++ Score: {chrf_score:.2f}\")\n",
    "    \n",
    "    results_df = pd.DataFrame({\n",
    "        \"telugu_sentence\": sources,\n",
    "        \"tamil_sentence\": references,\n",
    "        \"tamil_hypothesis\": hypotheses,\n",
    "        \"chrf_score\": [chrf_score] * len(sources)\n",
    "    })\n",
    "    results_df.to_csv(CHRF_OUTPUT_PATH, index=False)\n",
    "    logger.info(f\"chrF++ results saved to {CHRF_OUTPUT_PATH}\")\n",
    "    \n",
    "    return chrf_score\n",
    "    \n",
    "# Compute TER score\n",
    "def compute_ter(test_data, sources, references, hypotheses):\n",
    "    logger.info(\"Computing TER score...\")\n",
    "    \n",
    "    refs_list = [[ref] for ref in references]\n",
    "    \n",
    "    ter = corpus_ter(hypotheses, refs_list)\n",
    "    ter_score = ter.score\n",
    "    logger.info(f\"TER Score: {ter_score:.2f}\")\n",
    "    \n",
    "    results_df = pd.DataFrame({\n",
    "        \"telugu_sentence\": sources,\n",
    "        \"tamil_sentence\": references,\n",
    "        \"tamil_hypothesis\": hypotheses,\n",
    "        \"ter_score\": [ter_score] * len(sources)\n",
    "    })\n",
    "    results_df.to_csv(TER_OUTPUT_PATH, index=False)\n",
    "    logger.info(f\"TER results saved to {TER_OUTPUT_PATH}\")\n",
    "    \n",
    "    return ter_score\n",
    "\n",
    "# Compute BERTScore\n",
    "def compute_bertscore(test_data, sources, references, hypotheses):\n",
    "    logger.info(\"Computing BERTScore...\")\n",
    "    \n",
    "    P, R, F1 = bert_score(\n",
    "        hypotheses,\n",
    "        references,\n",
    "        lang=\"ta\",\n",
    "        model_type=\"bert-base-multilingual-cased\",\n",
    "        device=device,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    avg_f1 = F1.mean().item()\n",
    "    logger.info(f\"BERTScore F1: {avg_f1:.4f}\")\n",
    "    \n",
    "    bert_f1_scores = [f1.item() for f1 in F1]\n",
    "    \n",
    "    results_df = pd.DataFrame({\n",
    "        \"telugu_sentence\": sources,\n",
    "        \"tamil_sentence\": references,\n",
    "        \"tamil_hypothesis\": hypotheses,\n",
    "        \"bertscore_f1\": bert_f1_scores\n",
    "    })\n",
    "    results_df.to_csv(BERTSCORE_OUTPUT_PATH, index=False)\n",
    "    logger.info(f\"BERTScore results saved to {BERTSCORE_OUTPUT_PATH}\")\n",
    "    \n",
    "    return avg_f1\n",
    "\n",
    "# Compute COMET score\n",
    "def compute_comet(test_data, sources, references, hypotheses):\n",
    "    if not comet_available:\n",
    "        logger.warning(\"COMET not available. Skipping COMET evaluation.\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(\"Computing COMET score...\")\n",
    "    \n",
    "    model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "    model = load_from_checkpoint(model_path)\n",
    "    model.to(device)\n",
    "    \n",
    "    data = []\n",
    "    for src, hyp, ref in zip(sources, hypotheses, references):\n",
    "        data.append({\n",
    "            \"src\": src,\n",
    "            \"mt\": hyp,\n",
    "            \"ref\": ref\n",
    "        })\n",
    "    \n",
    "    logger.info(\"Running COMET evaluation...\")\n",
    "    model_output = model.predict(data, batch_size=8, gpus=1 if device == \"cuda\" else 0)\n",
    "    comet_scores = model_output.scores\n",
    "    avg_comet = model_output.system_score\n",
    "    \n",
    "    logger.info(f\"COMET Score: {avg_comet:.4f}\")\n",
    "    \n",
    "    results_df = pd.DataFrame({\n",
    "        \"telugu_sentence\": sources,\n",
    "        \"tamil_sentence\": references,\n",
    "        \"tamil_hypothesis\": hypotheses,\n",
    "        \"comet_score\": comet_scores\n",
    "    })\n",
    "    results_df.to_csv(COMET_OUTPUT_PATH, index=False)\n",
    "    logger.info(f\"COMET results saved to {COMET_OUTPUT_PATH}\")\n",
    "    \n",
    "    return avg_comet\n",
    "\n",
    "# Main evaluation function\n",
    "def evaluate_model():\n",
    "    test_data, sources, references, hypotheses = generate_translations(test_dataset, NUM_SAMPLES)\n",
    "    \n",
    "    bleu_score = compute_bleu(test_data, sources, references, hypotheses)\n",
    "    bertscore_f1 = compute_bertscore(test_data, sources, references, hypotheses)\n",
    "    chrf_score = compute_chrf(test_data, sources, references, hypotheses)\n",
    "    ter_score = compute_ter(test_data, sources, references, hypotheses)\n",
    "    \n",
    "    comet_score = None\n",
    "    if comet_available:\n",
    "        comet_score = compute_comet(test_data, sources, references, hypotheses)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Number of samples: {len(sources)}\")\n",
    "    print(f\"BLEU Score: {bleu_score:.2f}\")\n",
    "    print(f\"chrF++ Score: {chrf_score:.2f}\")\n",
    "    print(f\"TER Score: {ter_score:.2f} (lower is better)\")\n",
    "    print(f\"BERTScore F1: {bertscore_f1:.4f}\")\n",
    "    if comet_score is not None:\n",
    "        print(f\"COMET Score: {comet_score:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    test_input = \"హాయ్, మీరు ఎలా ఉన్నారు?\"  # \"Hi, how are you?\"\n",
    "    translated_text = mbart_translate_text(test_input, debug=True)\n",
    "    print(f\"\\nTest Translation:\")\n",
    "    print(f\"Source (Telugu): {test_input}\")\n",
    "    print(f\"Target (Tamil): {translated_text}\")\n",
    "    \n",
    "    return {\n",
    "        \"bleu\": bleu_score,\n",
    "        \"chrf\": chrf_score,\n",
    "        \"ter\": ter_score,\n",
    "        \"bertscore\": bertscore_f1,\n",
    "        \"comet\": comet_score\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
