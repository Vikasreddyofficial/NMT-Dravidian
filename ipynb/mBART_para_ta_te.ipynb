{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2c2dc8d",
   "metadata": {},
   "source": [
    "#Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d460f33-db8d-44e4-a92b-1b2f72b51d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "DataFrame shape: (2856, 21)\n",
      "Sample data:\n",
      "    Index                                            English  English Tokens  \\\n",
      "0      0  Every individual, every business, sets a New Y...             145   \n",
      "1      1  COVID AND MANPOWER: It is the power of manpowe...              71   \n",
      "2      2  CAUTION: It's also worth noting that a new Cor...              70   \n",
      "3      3  GROUP CAPTAIN VARUN SINGH: In the recent trage...             111   \n",
      "4      4  DISCUSSION ON EXAMS: Every year, I discuss top...              68   \n",
      "\n",
      "   English Sentences  English Words  \\\n",
      "0                  4             91   \n",
      "1                  2             43   \n",
      "2                  3             45   \n",
      "3                  5             66   \n",
      "4                  3             45   \n",
      "\n",
      "                                             Kannada  Kannada Tokens  \\\n",
      "0  ಪ್ರತಿ ವ್ಯಕ್ತಿಯೂ, ಪ್ರತಿ ವೃತ್ತಿಯೂ, ಮುಂದಿನ ವರ್ಷದಲ...             145   \n",
      "1  ಕೋವಿಡ್ ಮತ್ತು ಮಾನವಶಕ್ತಿ: ಭಾರತವು 100 ವರ್ಷಗಳಲ್ಲಿಯ...              67   \n",
      "2  ಎಚ್ಚರಿಕೆ: ಹೊಸ ಕೊರೋನಾ ರೂಪಾಂತರವು ಬಂದಿರುವುದು ಸಹ ಗ...              60   \n",
      "3  ಗ್ರೂಪ್ ಕ್ಯಾಪ್ಟನ್ ವರುಣ್ ಸಿಂಗ್: ಇತ್ತೀಚೆಗೆ ತಮಿಳುನ...             109   \n",
      "4  ಪರೀಕ್ಷೆಗಳ ಕುರಿತು ಚರ್ಚೆ: ಪ್ರತಿ ವರ್ಷ, ನಾನು ಪರೀಕ್...              56   \n",
      "\n",
      "   Kannada Sentences  Kannada Words  \\\n",
      "0                  5             83   \n",
      "1                  2             29   \n",
      "2                  3             31   \n",
      "3                  5             54   \n",
      "4                  3             32   \n",
      "\n",
      "                                           Malayalam  ...  \\\n",
      "0  ഓരോ വ്യക്തിയും, ഓരോ വ്യവസായവും, അടുത്ത വർഷം കൂ...  ...   \n",
      "1  കോവിഡും മനുഷ്യശക്തിയും 100 വർഷത്തിനിടയിലെ ഏറ്റ...  ...   \n",
      "2  ജാഗ്രത: കൊറോണയുടെ പുതിയ വകഭേദം വന്നിരിക്കുന്നു...  ...   \n",
      "3  ഗ്രൂപ്പ് ക്യാപ്റ്റൻ വരുൺ സിങ്: തമിഴ്നാട്ടിൽ അട...  ...   \n",
      "4  പരീക്ഷകളെക്കുറിച്ചുള്ള ചർച്ച എല്ലാ വർഷവും, പരീ...  ...   \n",
      "\n",
      "   Malayalam Sentences  Malayalam Words  \\\n",
      "0                    6               80   \n",
      "1                    2               28   \n",
      "2                    4               28   \n",
      "3                    5               49   \n",
      "4                    3               36   \n",
      "\n",
      "                                               Tamil Tamil Tokens  \\\n",
      "0  ஒவ்வொரு மனிதரும், ஒவ்வொரு வர்த்தகமும், ஏதாவது ...          150   \n",
      "1  கொவிட் மற்றும் மனித சக்திஃ 100 ஆண்டுகளில் ஏற்ப...           54   \n",
      "2  எச்சரிக்கை : புதிய உருமாறிய கொரோனா வந்திருப்பத...           69   \n",
      "3  குரூப் கேப்டன் வருண் சிங் : தமிழ்நாட்டில் அண்ம...           96   \n",
      "4  தேர்வுகள் குறித்த விவாதம் : தேர்வு எனும் மன அழ...           65   \n",
      "\n",
      "   Tamil Sentences  Tamil Words  \\\n",
      "0                6           82   \n",
      "1                2           29   \n",
      "2                2           32   \n",
      "3                5           54   \n",
      "4                3           38   \n",
      "\n",
      "                                              Telugu Telugu Tokens  \\\n",
      "0  ప్రతి వ్యక్తి, ప్రతి వ్యాపారం వచ్చే సంవత్సరం మ...           171   \n",
      "1  కోవిడ్, మానవ వనరులు: మానవ వనరుల బలంతోనే భారతదే...            60   \n",
      "2  అప్రమత్తం: కరోనా కొత్త వేరియెంట్ ఇప్పుడు ప్రపం...            68   \n",
      "3  గ్రూప్ కెప్టెన్ వరుణ్ సింగ్: ఇటీవల తమిళనాడులో ...            89   \n",
      "4  పరీక్షలపై చర్చ: ప్రతీ సంవత్సరం నేను పరీక్షలకు ...            55   \n",
      "\n",
      "   Telugu Sentences  Telugu Words  \n",
      "0                 7            98  \n",
      "1                 2            30  \n",
      "2                 4            31  \n",
      "3                 5            54  \n",
      "4                 3            33  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "Train dataset size: 2570\n",
      "Test dataset size: 286\n",
      "mBART - Initial tokenizer vocab size: 250054\n",
      "mBART - Initial model output vocab size: 250054\n",
      "mBART - Vocab sizes match, no adjustment needed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abe792b777d4f40ba76b1adb9b56d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e655c7956d4f14a6958c9db2cbe88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/286 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mBART - Tokenized train sample: {'Index': 2716, 'English': 'By adopting the drip irrigation method, about 74 million kWh of energy was saved in a year. Interlinking of 13 rivers and water transfer in Sardar Sarovar Canal Project, continuous monitoring of Sardar Sarovar Project accelerated the implementation. Due to this, where in 2000-01, 45.12% and 28.59% of electricity were consumed in the agriculture and industrial sector respectively, it changed to 21.10% in the agriculture sector and 35.26 percent in the industrial sector in 2008-09 . These savings were due to the lesser use of running a motor to extract groundwater. Also, less pumping was required due to the rise in water level. This saving was equivalent to 15,459 million tonnes of carbon emissions.', 'English Tokens': 184, 'English Sentences': 6, 'English Words': 114, 'Kannada': 'ಹನಿ ನೀರಾವರಿ ವಿಧಾನವನ್ನು ಅಳವಡಿಸಿಕೊಳ್ಳುವ ಮೂಲಕ, ಒಂದು ವರ್ಷದಲ್ಲಿ ಸುಮಾರು 74 ಮಿಲಿಯನ್ ಕಿವ್ಯಾ. ವಿದ್ಯುತ್ ಉಳಿಸಲಾಗಿದೆ. 13 ನದಿಗಳ ಜೋಡಣೆ ಮತ್ತು ಸರ್ದಾರ್ ಸರೋವರ ಕಾಲುವೆ ಯೋಜನೆಯಲ್ಲಿ ನೀರು ವರ್ಗಾವಣೆ, ಸರ್ದಾರ್ ಸರೋವರ ಯೋಜನೆಯ ನಿರಂತರ ಮೇಲ್ವಿಚಾರಣೆ ಅನುಷ್ಠಾನವನ್ನು ವೇಗಗೊಳಿಸಿತು. 2000-01 ರಲ್ಲಿ ಕೃಷಿ ಮತ್ತು ಕೈಗಾರಿಕಾ ವಲಯದಲ್ಲಿ ಕ್ರಮವಾಗಿ ಶೇ 45.12 ಮತ್ತು ಶೇ 28.59ರಷ್ಟು ವಿದ್ಯುತ್ ಅನ್ನು ಬಳಸಲಾಗುತ್ತಿತ್ತು, ಇದು 2008- 09 ರಲ್ಲಿ ಕೃಷಿ ವಲಯದಲ್ಲಿ ಶೇ 21.10 ಮತ್ತು ಕೈಗಾರಿಕಾ ವಲಯದಲ್ಲಿ ಶೇ 35.26 ಕ್ಕೆ ಇಳಿಕೆಯಾಯಿತು. ಅಂತರ್ಜಲವನ್ನು ಹೊರತೆಗೆಯಲು ಮೋಟಾರುಗಳ ಕಡಿಮೆ ಬಳಕೆಯು ಈ ಉಳಿತಾಯಕ್ಕೆ ಕಾರಣವಾಗಿದೆ. ಅಲ್ಲದೆ, ನೀರಿನ ಮಟ್ಟ ಏರಿಕೆಯಿಂದಾಗಿ ಕಡಿಮೆ ಪಂಪಿಂಗ್ ಸಾಕಾಗುತ್ತದೆ. ಈ ಉಳಿತಾಯವು 15,459 ಮಿಲಿಯನ್ ಟನ್ ಇಂಗಾಲದ ಹೊರಸೂಸುವಿಕೆಗೆ ಸಮನಾಗಿತ್ತು.', 'Kannada Tokens': 173, 'Kannada Sentences': 7, 'Kannada Words': 83, 'Malayalam': 'സർദാർ സരോവർ കനാൽ പദ്ധതിയിൽ 13 നദികളെ ബന്ധി പ്പിക്കുന്നതും ജല കൈമാറ്റവും സർദാർ സരോവർ പദ്ധതിയുടെ തുടർച്ചയായ നിരീക്ഷണവും നിർവഹ ണം ത്വരിതപ്പെടുത്തി. ഇക്കാരണത്താൽ, 2000-01ൽ കാർഷിക മേഖലയിലും വ്യാവസായിക മേഖലയിലും യഥാക്രമം 45.12%, 28.59% വൈദ്യുതി ഉപയോഗിച്ചിരു ന്നിടത്ത് അത് കാർഷിക മേഖലയിൽ 21.10% ആയും 2008-09-ൽ വ്യാവസായിക മേഖലയിൽ 35.26 ശതമാന മായും മാറി. ഭൂഗർഭജലം ഊറ്റിയെടുക്കാൻ മോട്ടോർ പ്രവർത്തിപ്പിക്കുന്നത് കുറഞ്ഞതാണ് ഈ ലാഭത്തിന് കാരണം. കൂടാതെ, ജലനിരപ്പ് ഉയർന്നതിനാൽ കുറ ച്ച് പമ്പിംഗ് മാത്രമേ ആവശ്യമായിരുന്നുള്ളു. ഈ ലാ ഭം 15,459 ദശലക്ഷം ടൺ കാർബൺ ഉദ്ഗമനത്തിന് തുല്യമാണ്.', 'Malayalam Tokens': 179, 'Malayalam Sentences': 5, 'Malayalam Words': 67, 'Tamil Tokens': 184, 'Tamil Sentences': 7, 'Tamil Words': 78, 'Telugu Tokens': 157, 'Telugu Sentences': 7, 'Telugu Words': 79, 'input_ids': [250044, 127442, 11993, 201119, 10897, 9734, 4930, 43444, 6345, 235005, 7016, 61454, 58540, 4, 41368, 10890, 42665, 3769, 2861, 617, 64252, 218866, 9313, 5270, 213770, 234636, 938, 52602, 11449, 2798, 67769, 5, 702, 10860, 5944, 6819, 105457, 135962, 4, 130416, 9654, 2912, 7963, 28577, 8182, 106911, 184842, 3093, 101987, 5055, 19238, 12309, 128438, 107039, 4, 130416, 9654, 2912, 7963, 28577, 8182, 101987, 8285, 75061, 71779, 10753, 18572, 135962, 86143, 125956, 18806, 58540, 116683, 2690, 4548, 10175, 36937, 45237, 3153, 225119, 5, 116683, 154944, 4, 24631, 57836, 25650, 93420, 3791, 3576, 45669, 938, 120785, 8938, 2678, 5, 1530, 211814, 938, 213770, 31142, 234636, 21651, 147093, 80517, 4, 2021, 38118, 3769, 7245, 46751, 9, 31085, 2650, 163062, 2802, 5, 99732, 93420, 3791, 234636, 147093, 80517, 12511, 758, 12678, 9, 86357, 167072, 138, 158296, 12187, 9, 31085, 2650, 232120, 2802, 5, 77690, 3770, 5427, 19238, 12309, 12095, 9000, 220230, 6, 40983, 50667, 12309, 75774, 50093, 61454, 62533, 163062, 61454, 58540, 4, 234636, 938, 52602, 11449, 2798, 59920, 5, 106911, 4548, 94593, 77211, 5966, 61454, 100710, 43427, 4, 19238, 12309, 12095, 9000, 7114, 108563, 174238, 26656, 163062, 2802, 5, 116683, 46697, 4, 423, 4, 170290, 47725, 27957, 7016, 2913, 6, 54174, 149903, 64804, 68047, 106478, 116669, 16043, 938, 163062, 33234, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [250045, 99569, 2538, 8711, 7377, 3991, 4701, 144170, 12916, 143968, 190822, 11055, 29446, 112752, 71711, 1296, 15124, 32297, 164170, 14076, 1296, 6914, 48497, 42259, 108755, 1092, 2195, 14314, 188935, 5, 702, 9327, 12091, 1944, 59006, 26649, 43624, 16577, 4, 71187, 14314, 2645, 8653, 19835, 45997, 123178, 2106, 186373, 8703, 6204, 21104, 127953, 4873, 5, 71187, 14314, 2645, 8653, 19835, 45997, 123178, 143968, 2078, 6254, 2894, 16736, 21289, 72738, 36111, 18772, 21559, 35396, 5, 1767, 6, 75924, 4230, 1944, 214060, 1949, 3576, 55667, 6, 93982, 2584, 179727, 178747, 2678, 5, 1530, 29094, 208566, 31846, 107630, 21746, 178747, 12511, 758, 12678, 9532, 203084, 216964, 1092, 2021, 1104, 3894, 6, 93982, 2584, 179727, 178747, 7245, 963, 113038, 4, 208566, 31846, 107630, 21746, 178747, 2273, 5, 4046, 100444, 3289, 8197, 63653, 9380, 5, 117721, 64651, 18472, 204279, 4276, 24767, 195617, 208213, 22655, 43923, 20319, 137456, 216964, 1092, 63653, 115550, 7377, 57567, 106104, 5, 80577, 22808, 117721, 64651, 18472, 204279, 3071, 40661, 1092, 30049, 2894, 4701, 11055, 40081, 105970, 81462, 5253, 63653, 9380, 5, 19588, 423, 4, 170290, 55216, 10800, 18582, 6, 32750, 19772, 2089, 6, 44204, 128986, 11171, 2195, 14314, 3520, 228586, 1092, 5, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n",
      "Further fine-tuning mBART on paragraph data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_358643/2287976881.py:123: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  mbart_trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='966' max='966' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [966/966 09:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.095700</td>\n",
       "      <td>1.773528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.979100</td>\n",
       "      <td>1.751427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.912700</td>\n",
       "      <td>1.748207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mBART - Saved tokenizer vocab size: 250054\n",
      "mBART - Saved model output vocab size: 250054\n",
      "Tokenized Input IDs: [[250044, 93860, 69535, 56150, 4, 93860, 209542, 26656, 4, 214978, 199044, 26437, 124002, 108048, 81392, 4, 35235, 120785, 8938, 35576, 938, 175309, 79209, 15413, 51153, 42665, 3769, 174611, 938, 203312, 84194, 64875, 5, 118511, 10860, 4551, 214771, 113627, 64371, 9, 938, 120785, 8938, 129582, 42756, 5944, 2913, 173039, 58540, 227228, 48068, 130110, 6490, 7083, 91677, 5, 59386, 17056, 69179, 17289, 10860, 175331, 13764, 24395, 88310, 12309, 81069, 54174, 4, 241019, 938, 13128, 6, 83862, 151640, 227778, 224066, 228591, 49422, 15377, 24183, 32105, 91585, 2690, 133523, 5894, 144678, 12095, 25683, 9313, 11830, 45237, 2798, 4548, 14622, 152711, 67442, 22173, 203312, 173812, 5, 40252, 139864, 64804, 39311, 6736, 11586, 5894, 86604, 938, 221659, 5414, 49604, 5, 49422, 120785, 18806, 238349, 129582, 42756, 5944, 2913, 14233, 4551, 37368, 6001, 22070, 5944, 10753, 2690, 11449, 2912, 4875, 9696, 39035, 3093, 9734, 73807, 4, 31107, 6390, 10175, 94116, 34987, 4, 46018, 12784, 7539, 4, 3219, 39290, 2802, 61138, 3791, 155837, 21651, 227293, 4, 46756, 188241, 142272, 4, 14780, 188241, 6819, 10860, 4930, 9313, 2798, 4, 12359, 50327, 7114, 61454, 65808, 94281, 938, 67844, 118511, 44015, 91677, 5, 2]]\n",
      "Raw Output IDs: [2, 250045, 22883, 71259, 4, 22883, 113385, 1092, 116044, 5738, 91641, 5635, 6258, 13973, 73677, 18089, 159858, 1296, 100147, 13367, 93683, 2645, 69273, 1092, 142635, 5, 52844, 218972, 211333, 161920, 64371, 159858, 1296, 69502, 3520, 99238, 14839, 129461, 8729, 12217, 1886, 29446, 41878, 4873, 5, 80833, 192055, 132095, 93090, 13249, 162136, 4276, 203397, 1296, 4, 52994, 69502, 40209, 18793, 4510, 4701, 30668, 25028, 33242, 4510, 2078, 24722, 9573, 3227, 16783, 4918, 46551, 73699, 31260, 225476, 5, 69502, 110096, 2584, 1767, 154534, 5738, 220633, 4856, 228230, 169156, 5, 80833, 112752, 13249, 99238, 48771, 16351, 2195, 42795, 6204, 2635, 3409, 136571, 5688, 103964, 121094, 5688, 4, 156158, 238761, 4, 45822, 204392, 3289, 4, 204674, 202891, 133725, 1092, 117109, 4, 91641, 34568, 1361, 4, 2195, 34568, 9995, 18192, 89838, 119503, 11055, 4, 87324, 3071, 106203, 52379, 60078, 7713, 52844, 218972, 232897, 4873, 5, 2]\n",
      "Decoded with special tokens: </s>te_IN ప్రతి వ్యక్తి, ప్రతి వ్యాపారం ఏదో ఒక గొప్పదాన్ని చేయటానికి కొత్త సంవత్సరంలో అభివృద్ధి కోసం తీర్మానం చేస్తుంది. ప్రధానమంత్రి నరేంద్ర మోదీ 2021 సంవత్సరంలో ప్రజలతో చివరి మనస్సు వాయిస్ ద్వారా మాట్లాడారు. గత ఏడు సంవత్సరాలుగా ఆయన ప్రయాణాలు సమాజంలో, దేశ ప్రజల్లో మెరుగైన పనితీరును ఎలా ప్రేరేపించి చేశాయో వివరించారు. ప్రజల శక్తికి ఈ వేదిక ఒక సాధనంగా తయారైంది. గత ఏడాది ఆయన చివరి మాటల్లో ఆశాదిక అమృత్ మహోత్సవ్, భారతీయ సంస్కృతి, స్వచ్ఛత, ఒకరి జీవితంలో సాహిత్యం విలువ, గొప్ప కలలు, ఆ కలలను సాకారం చేసుకోవడం, శ్రమించడం వంటి వాటిపై ప్రధానమంత్రి చర్చించారు.</s>\n",
      "mBART Translation: ప్రతి వ్యక్తి, ప్రతి వ్యాపారం ఏదో ఒక గొప్పదాన్ని చేయటానికి కొత్త సంవత్సరంలో అభివృద్ధి కోసం తీర్మానం చేస్తుంది. ప్రధానమంత్రి నరేంద్ర మోదీ 2021 సంవత్సరంలో ప్రజలతో చివరి మనస్సు వాయిస్ ద్వారా మాట్లాడారు. గత ఏడు సంవత్సరాలుగా ఆయన ప్రయాణాలు సమాజంలో, దేశ ప్రజల్లో మెరుగైన పనితీరును ఎలా ప్రేరేపించి చేశాయో వివరించారు. ప్రజల శక్తికి ఈ వేదిక ఒక సాధనంగా తయారైంది. గత ఏడాది ఆయన చివరి మాటల్లో ఆశాదిక అమృత్ మహోత్సవ్, భారతీయ సంస్కృతి, స్వచ్ఛత, ఒకరి జీవితంలో సాహిత్యం విలువ, గొప్ప కలలు, ఆ కలలను సాకారం చేసుకోవడం, శ్రమించడం వంటి వాటిపై ప్రధానమంత్రి చర్చించారు.\n",
      "Training log plot saved as 'training_loss_plot.png' in the output directory.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import MBart50TokenizerFast, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "import logging\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "log_file = os.path.join(\"./mbart_finetuned_paragraphs\", \"training_logs.csv\")\n",
    "os.makedirs(\"./mbart_finetuned_paragraphs\", exist_ok=True)\n",
    "\n",
    "with open(log_file, mode='w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"epoch\", \"step\", \"training_loss\", \"validation_loss\", \"learning_rate\", \"timestamp\"])\n",
    "\n",
    "\n",
    "class CustomLoggingCallback(transformers.TrainerCallback):\n",
    "    def __init__(self, log_file):\n",
    "        self.log_file = log_file\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is None:\n",
    "            return\n",
    "        epoch = state.epoch\n",
    "        step = state.global_step\n",
    "        training_loss = logs.get(\"loss\", None)\n",
    "        validation_loss = logs.get(\"eval_loss\", None)\n",
    "        learning_rate = logs.get(\"learning_rate\", None)\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        with open(self.log_file, mode='a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([epoch, step, training_loss, validation_loss, learning_rate, timestamp])\n",
    "\n",
    "# Load paragraph dataset\n",
    "df = pd.read_csv('./Paragraphs.csv') \n",
    "df1 = df.copy()\n",
    "\n",
    "\n",
    "print(\"DataFrame shape:\", df1.shape)\n",
    "print(\"Sample data:\\n\", df1.head(5))\n",
    "\n",
    "\n",
    "df1 = df1.rename(columns={\"Tamil\": \"ta\", \"Telugu\": \"te\"})\n",
    "\n",
    "\n",
    "dataset = Dataset.from_pandas(df1)\n",
    "split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "split_dataset = DatasetDict({\"train\": split_dataset[\"train\"], \"test\": split_dataset[\"test\"]})\n",
    "print(\"Train dataset size:\", len(split_dataset[\"train\"]))\n",
    "print(\"Test dataset size:\", len(split_dataset[\"test\"]))\n",
    "\n",
    "# Load previously fine-tuned mBART model and tokenizer\n",
    "MBART_MODEL_PATH = \"./mbart_finetuned3\"\n",
    "mbart_tokenizer = MBart50TokenizerFast.from_pretrained(MBART_MODEL_PATH, src_lang=\"ta_IN\", tgt_lang=\"te_IN\")\n",
    "mbart_model = AutoModelForSeq2SeqLM.from_pretrained(MBART_MODEL_PATH).to(device)\n",
    "\n",
    "\n",
    "mbart_vocab_size_tokenizer = len(mbart_tokenizer)\n",
    "mbart_vocab_size_model = mbart_model.get_output_embeddings().weight.size(0)\n",
    "print(\"mBART - Initial tokenizer vocab size:\", mbart_vocab_size_tokenizer)\n",
    "print(\"mBART - Initial model output vocab size:\", mbart_vocab_size_model)\n",
    "\n",
    "\n",
    "if mbart_vocab_size_tokenizer != mbart_vocab_size_model:\n",
    "    print(f\"Warning: mBART vocab size mismatch (Tokenizer: {mbart_vocab_size_tokenizer}, Model: {mbart_vocab_size_model}). Adjusting model embeddings.\")\n",
    "    mbart_model.resize_token_embeddings(mbart_vocab_size_tokenizer)\n",
    "    print(\"Post-resize model vocab size:\", mbart_model.get_output_embeddings().weight.size(0))\n",
    "else:\n",
    "    print(\"mBART - Vocab sizes match, no adjustment needed.\")\n",
    "\n",
    "# Preprocessing function for paragraphs\n",
    "def mbart_preprocess_function(examples):\n",
    "    inputs = [ta_text for ta_text in examples[\"ta\"]]\n",
    "    targets = [te_text for te_text in examples[\"te\"]]\n",
    "    model_inputs = mbart_tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    with mbart_tokenizer.as_target_tokenizer():\n",
    "        labels = mbart_tokenizer(targets, max_length=512, truncation=True, padding=\"max_length\").input_ids\n",
    "    labels = [[-100 if token == mbart_tokenizer.pad_token_id else token for token in seq] for seq in labels]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "mbart_tokenized_datasets = split_dataset.map(\n",
    "    mbart_preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=500,\n",
    "    remove_columns=[\"ta\", \"te\"]\n",
    ")\n",
    "print(\"mBART - Tokenized train sample:\", mbart_tokenized_datasets[\"train\"][0])\n",
    "\n",
    "# Training arguments for further fine-tuning\n",
    "mbart_training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mbart_finetuned_paragraphs\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=3,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    logging_steps=100,\n",
    "    save_steps=1000\n",
    ")\n",
    "\n",
    "\n",
    "mbart_data_collator = DataCollatorForSeq2Seq(mbart_tokenizer, model=mbart_model)\n",
    "mbart_trainer = Seq2SeqTrainer(\n",
    "    model=mbart_model,\n",
    "    args=mbart_training_args,\n",
    "    train_dataset=mbart_tokenized_datasets[\"train\"],\n",
    "    eval_dataset=mbart_tokenized_datasets[\"test\"],\n",
    "    tokenizer=mbart_tokenizer,\n",
    "    data_collator=mbart_data_collator,\n",
    "    callbacks=[CustomLoggingCallback(log_file)]\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"Further fine-tuning mBART on paragraph data...\")\n",
    "mbart_trainer.train()\n",
    "\n",
    "\n",
    "mbart_trainer.save_model(\"./mbart_finetuned_paragraphs\")\n",
    "mbart_tokenizer.save_pretrained(\"./mbart_finetuned_paragraphs\")\n",
    "\n",
    "\n",
    "mbart_saved_model = AutoModelForSeq2SeqLM.from_pretrained(\"./mbart_finetuned_paragraphs\").to(device)\n",
    "mbart_saved_tokenizer = MBart50TokenizerFast.from_pretrained(\"./mbart_finetuned_paragraphs\", src_lang=\"ta_IN\", tgt_lang=\"te_IN\")\n",
    "print(\"mBART - Saved tokenizer vocab size:\", len(mbart_saved_tokenizer))\n",
    "print(\"mBART - Saved model output vocab size:\", mbart_saved_model.get_output_embeddings().weight.size(0))\n",
    "\n",
    "# Test translation with debugging\n",
    "def mbart_translate_text(input_text, debug=False):\n",
    "    inputs = mbart_saved_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True).to(device)\n",
    "    if debug:\n",
    "        print(\"Tokenized Input IDs:\", inputs[\"input_ids\"].tolist())\n",
    "    outputs = mbart_saved_model.generate(\n",
    "        **inputs,\n",
    "        max_length=1024,\n",
    "        min_length=50,\n",
    "        num_beams=5,\n",
    "        early_stopping=False,\n",
    "        length_penalty=1.0,\n",
    "        no_repeat_ngram_size=3,\n",
    "        forced_bos_token_id=mbart_saved_tokenizer.lang_code_to_id[\"te_IN\"]\n",
    "    )\n",
    "    if debug:\n",
    "        print(\"Raw Output IDs:\", outputs[0].tolist())\n",
    "        print(\"Decoded with special tokens:\", mbart_saved_tokenizer.decode(outputs[0], skip_special_tokens=False))\n",
    "    decoded_output = mbart_saved_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return decoded_output.strip()\n",
    "\n",
    "# Test with provided sample paragraph\n",
    "input_text = \"\"\"ஒவ்வொரு மனிதரும், ஒவ்வொரு வர்த்தகமும், ஏதாவது சிறப்பானவற்றை செய்வதற்கு, வரும் ஆண்டில் மேம்பாட்டிற்கு புத்தாண்டில் தீர்மானம் எடுத்துக் கொள்கிறது. பிரதமர் நரேந்திர மோடி 2021-ம் ஆண்டில் கடைசி மனதின் குரல் மூலம் பொதுமக்களுடன் உரையாடினார். தனிநபர்களின் நற்குணங்களை எடுத்துரைப்பதுடன், சமுதாயம் மற்றும் நாட்டு மக்களிடையே சிறப்பாக செயல்பட கடந்த ஏழு ஆண்டுகளாக அவரின் இந்த பயணம் எவ்வாறு ஊக்கமளிக்கிறது என்பதை அவர் எடுத்துக் கூறினார். மக்கள் சக்திக்கான கருவியாக இந்த தளம் உருவாகி உள்ளது. கடந்த ஆண்டின் அவருடைய கடைசி மனதின் குரலில் ஆசாதிகா அமிர்த மகோத்சவம், இந்திய கலாச்சாரம், தூய்மை, ஒருவரது வாழ்க்கையில் இலக்கியத்தின் மதிப்பு, பெரிய கனவு கண்டு, அந்த கனவுகளை நனவாக்க, உழைப்பதன் முக்கியத்துவம் குறித்து பிரதமர் பேசினார்.\"\"\"\n",
    "translated_text = mbart_translate_text(input_text, debug=True)\n",
    "print(\"mBART Translation:\", translated_text)\n",
    "\n",
    "\n",
    "def plot_training_logs(log_file):\n",
    "    logs = pd.read_csv(log_file)\n",
    "    train_logs = logs[logs['training_loss'].notnull()]\n",
    "    valid_logs = logs[logs['validation_loss'].notnull()]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if not train_logs.empty:\n",
    "        plt.plot(train_logs['step'], train_logs['training_loss'], label='Training Loss', marker='o')\n",
    "    if not valid_logs.empty:\n",
    "        plt.plot(valid_logs['step'], valid_logs['validation_loss'], label='Validation Loss', marker='s')\n",
    "    \n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Over Time (Paragraph Fine-Tuning)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(\"./mbart_finetuned_paragraphs\", \"training_loss_plot.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Generate plot\n",
    "plot_training_logs(log_file)\n",
    "print(\"Training log plot saved as 'training_loss_plot.png' in the output directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ec58ec",
   "metadata": {},
   "source": [
    "#Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe97736c-f420-4b05-a8c8-d6f079a21b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 14:39:07.544498: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-10 14:39:07.556226: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746868147.569925  364523 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746868147.574246  364523 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746868147.584651  364523 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746868147.584667  364523 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746868147.584669  364523 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746868147.584670  364523 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-10 14:39:07.588528: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Tamil to Telugu Paragraph Translator</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Tamil paragraph below to translate to Telugu (type 'exit' to stop).\n",
      "Tip: Paste multi-line paragraphs and press Enter twice to submit.\n",
      "Tamil Paragraph Input (press Enter twice to submit):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m             display(HTML(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<p style=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor: red;\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>Error during translation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m</p>\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Run the translator\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m translate_interactively()\n",
      "Cell \u001b[0;32mIn[1], line 46\u001b[0m, in \u001b[0;36mtranslate_interactively\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTamil Paragraph Input (press Enter twice to submit):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m()\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m line \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m lines:  \u001b[38;5;66;03m# If there's already input, an empty line signals end\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nmt/lib/python3.12/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1286\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1287\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/nmt/lib/python3.12/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " exit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import MBart50TokenizerFast, AutoModelForSeq2SeqLM\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "MODEL_PATH = \"./mbart_finetuned_paragraphs\"  \n",
    "mbart_saved_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH).to(device)\n",
    "mbart_saved_tokenizer = MBart50TokenizerFast.from_pretrained(MODEL_PATH, src_lang=\"ta_IN\", tgt_lang=\"te_IN\")\n",
    "\n",
    "# Translation function for paragraphs\n",
    "def mbart_translate_text(input_text, debug=False):\n",
    "    inputs = mbart_saved_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True).to(device)\n",
    "    if debug:\n",
    "        print(\"Tokenized Input IDs:\", inputs[\"input_ids\"].tolist())\n",
    "    outputs = mbart_saved_model.generate(\n",
    "        **inputs,\n",
    "        max_length=1024,\n",
    "        min_length=50,\n",
    "        num_beams=5,\n",
    "        early_stopping=False,\n",
    "        length_penalty=1.0,\n",
    "        no_repeat_ngram_size=3,\n",
    "        forced_bos_token_id=mbart_saved_tokenizer.lang_code_to_id[\"te_IN\"]\n",
    "    )\n",
    "    if debug:\n",
    "        print(\"Raw Output IDs:\", outputs[0].tolist())\n",
    "        print(\"Decoded with special tokens:\", mbart_saved_tokenizer.decode(outputs[0], skip_special_tokens=False))\n",
    "    decoded_output = mbart_saved_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return decoded_output.strip()\n",
    "\n",
    "\n",
    "def translate_interactively():\n",
    "    display(HTML(\"<h3>Tamil to Telugu Paragraph Translator</h3>\"))\n",
    "    print(\"Enter Tamil paragraph below to translate to Telugu (type 'exit' to stop).\")\n",
    "    print(\"Tip: Paste multi-line paragraphs and press Enter twice to submit.\")\n",
    "    \n",
    "    while True:\n",
    "       \n",
    "        lines = []\n",
    "        print(\"Tamil Paragraph Input (press Enter twice to submit):\")\n",
    "        while True:\n",
    "            line = input()\n",
    "            if line == \"\":\n",
    "                if lines:  \n",
    "                    break\n",
    "                else: \n",
    "                    continue\n",
    "            lines.append(line)\n",
    "        \n",
    "       \n",
    "        user_input = \" \".join(lines).strip()\n",
    "        \n",
    "        #  exit condition\n",
    "        if user_input.lower() == \"exit\":\n",
    "            display(HTML(\"<p style='color: green;'>Exiting translator...</p>\"))\n",
    "            break\n",
    "        \n",
    "        if not user_input:\n",
    "            display(HTML(\"<p style='color: red;'>Please enter some text.</p>\"))\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            translated_text = mbart_translate_text(user_input, debug=False)  \n",
    "            display(HTML(f\"<p><b>Tamil Paragraph:</b><br>{user_input.replace(' ', '&nbsp;').replace('\\n', '<br>')}<br><br><b>Telugu Translation:</b><br>{translated_text.replace(' ', '&nbsp;').replace('\\n', '<br>')}</p>\"))\n",
    "        except Exception as e:\n",
    "            display(HTML(f\"<p style='color: red;'>Error during translation: {e}</p>\"))\n",
    "\n",
    "\n",
    "translate_interactively()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba42e2eb",
   "metadata": {},
   "source": [
    "#Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350de4f5-8199-4439-aa3c-cb59c66edaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[33 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 389, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 373, in main\n",
      "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n",
      "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 137, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     backend = _build_backend()\n",
      "  \u001b[31m   \u001b[0m               ^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 70, in _build_backend\n",
      "  \u001b[31m   \u001b[0m     obj = import_module(mod_path)\n",
      "  \u001b[31m   \u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/home/mca/anaconda3/envs/nmt/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "  \u001b[31m   \u001b[0m     return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1381, in _gcd_import\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1354, in _find_and_load\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1304, in _find_and_load_unlocked\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1381, in _gcd_import\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1354, in _find_and_load\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1325, in _find_and_load_unlocked\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 929, in _load_unlocked\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 994, in exec_module\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-9l4otyd7/overlay/lib/python3.12/site-packages/setuptools/__init__.py\", line 16, in <module>\n",
      "  \u001b[31m   \u001b[0m     import setuptools.version\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-9l4otyd7/overlay/lib/python3.12/site-packages/setuptools/version.py\", line 1, in <module>\n",
      "  \u001b[31m   \u001b[0m     import pkg_resources\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-9l4otyd7/overlay/lib/python3.12/site-packages/pkg_resources/__init__.py\", line 2172, in <module>\n",
      "  \u001b[31m   \u001b[0m     register_finder(pkgutil.ImpImporter, find_on_path)\n",
      "  \u001b[31m   \u001b[0m                     ^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model configuration...\n",
      "Loading model and tokenizer...\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:554: UserWarning: `num_beams` is set to None - defaulting to 1.\n",
      "  warnings.warn(\"`num_beams` is set to None - defaulting to 1.\", UserWarning)\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Loading dataset...\n",
      "Generating translations for 45 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: ['Tamil', 'Telugu']\n",
      "Dataset shape: (50, 2)\n",
      "First few rows:\n",
      "                                               Tamil  \\\n",
      "0  பெருந்தொற்று தீவிர வடிவம் பெறுவதை உணர்ந்துகொண்...   \n",
      "1  நான் தமிழகத்தைச் சேர்ந்த ஆனந்த். அரசு வேலைக்கா...   \n",
      "2  \"இந்திய இளைஞர்கள் நேர்மறையானவர்களாகவும், நடைமு...   \n",
      "3  பிரதமர் மோடி: நேற்று முன்தினம் வரை, அதாவது, ஜூ...   \n",
      "4  நமது சுதந்திரத்தின் 75-வது ஆண்டை இந்த ஆண்டு கொ...   \n",
      "\n",
      "                                              Telugu  \n",
      "0  ఈ మహమ్మారి తిరగబెడితే చాలా ప్రాణాంతకంగా ఉంటుంద...  \n",
      "1  నేను తమిళనాడుకి చెందిన ఆనంద్ని. మేము ప్రభుత్వ ...  \n",
      "2  భారతదేశ యువత చాలా సానుకూలమైన దృక్పథంలో, ఆచరణాత...  \n",
      "3  ప్రధాన మంత్రి మోదీ: జూన్ 1కి ముందు వరకు, అందరూ...  \n",
      "4  ఈ ఏడాది 75వ స్వాతంత్య్ర దినోత్సవం జరగబోతుంది. ...  \n",
      "Using Tamil column: Tamil\n",
      "Using Telugu column: Telugu\n",
      "Test dataset size: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating translations:   0%|                           | 0/45 [00:00<?, ?it/s]/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:554: UserWarning: `num_beams` is set to None - defaulting to 1.\n",
      "  warnings.warn(\"`num_beams` is set to None - defaulting to 1.\", UserWarning)\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/utils.py:1283: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Generating translations: 100%|██████████████████| 45/45 [00:50<00:00,  1.12s/it]\n",
      "Computing BLEU score...\n",
      "BLEU Score: 19.41\n",
      "BLEU results saved to ./para_results/bleu_evaluation_results.csv\n",
      "Computing BERTScore...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61808dae12d2497da01ed8b5c4309970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2150d9f87e334064b44c7d1e7d31b6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BERTScore F1: 0.8081\n",
      "BERTScore results saved to ./para_results/bertscore_evaluation_results.csv\n",
      "Computing chrF++ score...\n",
      "chrF++ Score: 41.34\n",
      "chrF++ results saved to ./para_results/chrf_evaluation_results.csv\n",
      "Computing TER score...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.42 seconds, 107.49 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TER Score: 82.13\n",
      "TER results saved to ./para_results/ter_evaluation_results.csv\n",
      "Computing COMET score...\n",
      "Downloading COMET model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17101111629e482c8a4c0d2f11a2e45a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Encoder model frozen.\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "Running COMET evaluation...\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX A6000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Predicting DataLoader 0: 100%|████████████████████| 6/6 [00:00<00:00,  6.26it/s]\n",
      "COMET Score: 0.8580\n",
      "COMET results saved to ./para_results/comet_evaluation_results.csv\n",
      "Tokenized Input IDs: [[250044, 93860, 69535, 56150, 4, 93860, 209542, 26656, 4, 214978, 199044, 26437, 124002, 108048, 81392, 4, 35235, 120785, 8938, 35576, 938, 175309, 79209, 15413, 51153, 42665, 3769, 174611, 938, 203312, 84194, 64875, 5, 118511, 10860, 4551, 214771, 113627, 64371, 9, 938, 120785, 8938, 129582, 42756, 5944, 2913, 173039, 58540, 227228, 48068, 130110, 6490, 7083, 91677, 5, 59386, 17056, 69179, 17289, 10860, 175331, 13764, 24395, 88310, 12309, 81069, 54174, 4, 241019, 938, 13128, 6, 83862, 151640, 227778, 224066, 228591, 49422, 15377, 24183, 32105, 91585, 2690, 133523, 5894, 144678, 12095, 25683, 9313, 11830, 45237, 2798, 4548, 14622, 152711, 67442, 22173, 203312, 173812, 5, 2]]\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:554: UserWarning: `num_beams` is set to None - defaulting to 1.\n",
      "  warnings.warn(\"`num_beams` is set to None - defaulting to 1.\", UserWarning)\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PARAGRAPH EVALUATION SUMMARY\n",
      "==================================================\n",
      "Number of samples: 45\n",
      "BLEU Score: 19.41\n",
      "chrF++ Score: 41.34\n",
      "TER Score: 82.13 (lower is better)\n",
      "BERTScore F1: 0.8081\n",
      "COMET Score: 0.8580\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Raw Output IDs: [2, 250045, 22883, 71259, 4, 22883, 113385, 1092, 116044, 5738, 91641, 5635, 6258, 13973, 73677, 18089, 159858, 1296, 100147, 13367, 93683, 2645, 69273, 1092, 142635, 5, 52844, 218972, 211333, 161920, 64371, 1296, 69502, 3520, 99238, 14839, 129461, 8729, 12217, 1886, 29446, 19203, 46773, 17161, 35396, 5, 80833, 192055, 132095, 93090, 10675, 162136, 4276, 227349, 6258, 4, 52994, 69502, 2078, 40803, 9573, 3227, 16783, 4918, 46551, 73699, 31260, 9573, 33913, 2502, 35396, 5, 2]\n",
      "Decoded with special tokens: </s>te_IN ప్రతి వ్యక్తి, ప్రతి వ్యాపారం ఏదో ఒక గొప్పదాన్ని చేయటానికి కొత్త సంవత్సరంలో అభివృద్ధి కోసం తీర్మానం చేస్తుంది. ప్రధానమంత్రి నరేంద్ర మోదీ 2021లో ప్రజలతో చివరి మనస్సు వాయిస్ ద్వారా సంభాషించారు. గత ఏడు సంవత్సరాలుగా తన ప్రయాణాలు సమాజాన్ని, దేశ ప్రజలను ఎంత ప్రేరేపించి చేశాయో ప్రస్తావించారు.</s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Paragraph Translation:\n",
      "Source (Tamil):\n",
      "ஒவ்வொரு மனிதரும், ஒவ்வொரு வர்த்தகமும், ஏதாவது சிறப்பானவற்றை செய்வதற்கு, வரும் ஆண்டில் மேம்பாட்டிற்கு புத்தாண்டில் தீர்மானம் எடுத்துக் கொள்கிறது. பிரதமர் நரேந்திர மோடி 2021-ம் ஆண்டில் கடைசி மனதின் குரல் மூலம் பொதுமக்களுடன் உரையாடினார். தனிநபர்களின் நற்குணங்களை எடுத்துரைப்பதுடன், சமுதாயம் மற்றும் நாட்டு மக்களிடையே சிறப்பாக செயல்பட கடந்த ஏழு ஆண்டுகளாக அவரின் இந்த பயணம் எவ்வாறு ஊக்கமளிக்கிறது என்பதை அவர் எடுத்துக் கூறினார்.\n",
      "Target (Telugu):\n",
      "ప్రతి వ్యక్తి, ప్రతి వ్యాపారం ఏదో ఒక గొప్పదాన్ని చేయటానికి కొత్త సంవత్సరంలో అభివృద్ధి కోసం తీర్మానం చేస్తుంది. ప్రధానమంత్రి నరేంద్ర మోదీ 2021లో ప్రజలతో చివరి మనస్సు వాయిస్ ద్వారా సంభాషించారు. గత ఏడు సంవత్సరాలుగా తన ప్రయాణాలు సమాజాన్ని, దేశ ప్రజలను ఎంత ప్రేరేపించి చేశాయో ప్రస్తావించారు.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bleu': 19.412822908062317,\n",
       " 'chrf': 41.335181730223454,\n",
       " 'ter': 82.13073005093379,\n",
       " 'bertscore': 0.8081279993057251,\n",
       " 'comet': 0.858004789882236}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "\n",
    "!pip install -q torch==2.3.1 torchvision==0.18.1\n",
    "!pip install -q transformers==4.41.2 datasets==2.20.0\n",
    "!pip install -q sacrebleu==2.3.1 pandas==2.2.2 numpy==1.25.2 tqdm==4.66.4\n",
    "!pip install -q bert-score==0.3.13\n",
    "!pip install -q protobuf==3.20.3 \n",
    "!pip install -q indic-nlp-library\n",
    "\n",
    "\n",
    "\n",
    "# Combines BLEU, BERTScore, COMET, chrF++ and TER evaluation metrics\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration, AutoConfig, AutoModelForSeq2SeqLM\n",
    "from sacrebleu import corpus_bleu, corpus_chrf, corpus_ter\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "\n",
    "try:\n",
    "    from comet import download_model, load_from_checkpoint\n",
    "    comet_available = True\n",
    "except ImportError:\n",
    "    comet_available = False\n",
    "    print(\"COMET not available. Will skip COMET evaluation.\")\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "MODEL_PATH = \"./mbart_finetuned_paragraphs\"  \n",
    "DATASET_PATH = \"./para/testing.csv\"  \n",
    "NUM_SAMPLES = 45  \n",
    "\n",
    "# Output paths\n",
    "os.makedirs(\"./para_results\", exist_ok=True)\n",
    "BLEU_OUTPUT_PATH = \"./para_results/bleu_evaluation_results.csv\"\n",
    "BERTSCORE_OUTPUT_PATH = \"./para_results/bertscore_evaluation_results.csv\" \n",
    "COMET_OUTPUT_PATH = \"./para_results/comet_evaluation_results.csv\"\n",
    "CHRF_OUTPUT_PATH = \"./para_results/chrf_evaluation_results.csv\"\n",
    "TER_OUTPUT_PATH = \"./para_results/ter_evaluation_results.csv\"\n",
    "\n",
    "# Load the model configuration first\n",
    "logger.info(\"Loading model configuration...\")\n",
    "config = AutoConfig.from_pretrained(MODEL_PATH)\n",
    "if hasattr(config, 'generation_config'):\n",
    "    if config.generation_config.early_stopping is None:\n",
    "        config.generation_config.early_stopping = True\n",
    "else:\n",
    "    config.early_stopping = True\n",
    "\n",
    "\n",
    "logger.info(\"Loading model and tokenizer...\")\n",
    "try:\n",
    "    mbart_saved_model = MBartForConditionalGeneration.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        config=config\n",
    "    ).to(device)\n",
    "except:\n",
    "    mbart_saved_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        config=config\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "if hasattr(mbart_saved_model, 'generation_config'):\n",
    "    mbart_saved_model.generation_config.early_stopping = True\n",
    "\n",
    "mbart_saved_tokenizer = MBart50TokenizerFast.from_pretrained(MODEL_PATH, src_lang=\"ta_IN\", tgt_lang=\"te_IN\")\n",
    "\n",
    "# Load the test dataset\n",
    "logger.info(\"Loading dataset...\")\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "print(f\"Dataset columns: {df.columns.tolist()}\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"First few rows:\\n{df.head()}\")\n",
    "\n",
    "\n",
    "tamil_col = None\n",
    "telugu_col = None\n",
    "\n",
    "# Common column name patterns to check\n",
    "tamil_patterns = ['tamil_sentence', 'tamil', 'source', 'src', 'Tamil', 'tamil_text']\n",
    "telugu_patterns = ['telugu_sentence', 'telugu', 'target', 'tgt', 'Telugu', 'telugu_text']\n",
    "\n",
    "for col in df.columns:\n",
    "    if any(pattern.lower() in col.lower() for pattern in tamil_patterns):\n",
    "        tamil_col = col\n",
    "    if any(pattern.lower() in col.lower() for pattern in telugu_patterns):\n",
    "        telugu_col = col\n",
    "\n",
    "if tamil_col is None or telugu_col is None:\n",
    "    raise ValueError(f\"Could not identify Tamil and Telugu columns. Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "print(f\"Using Tamil column: {tamil_col}\")\n",
    "print(f\"Using Telugu column: {telugu_col}\")\n",
    "\n",
    "\n",
    "df = df[[tamil_col, telugu_col]].dropna()\n",
    "\n",
    "df = df.rename(columns={tamil_col: 'tamil_sentence', telugu_col: 'telugu_sentence'})\n",
    "test_dataset = Dataset.from_pandas(df)\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "def indic_tokenize_text(text):\n",
    "    if not text or pd.isna(text):\n",
    "        return \"\"\n",
    "    return ' '.join(indic_tokenize.trivial_tokenize(text, lang='te'))\n",
    "\n",
    "# Translation function for paragraphs\n",
    "def mbart_translate_text(input_text, debug=False):\n",
    "    inputs = mbart_saved_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True).to(device)\n",
    "    if debug:\n",
    "        logger.info(f\"Tokenized Input IDs: {inputs['input_ids'].tolist()}\")\n",
    "    outputs = mbart_saved_model.generate(\n",
    "        **inputs,\n",
    "        max_length=1024,\n",
    "        min_length=50,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        length_penalty=1.0,\n",
    "        no_repeat_ngram_size=3,\n",
    "        forced_bos_token_id=mbart_saved_tokenizer.lang_code_to_id[\"te_IN\"]\n",
    "    )\n",
    "    if debug:\n",
    "        logger.info(f\"Raw Output IDs: {outputs[0].tolist()}\")\n",
    "        logger.info(f\"Decoded with special tokens: {mbart_saved_tokenizer.decode(outputs[0], skip_special_tokens=False)}\")\n",
    "    decoded_output = mbart_saved_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return decoded_output.strip()\n",
    "\n",
    "\n",
    "def generate_translations(dataset, num_samples=NUM_SAMPLES):\n",
    "    sources = []\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    \n",
    "    test_data = dataset.select(range(min(num_samples, len(dataset))))\n",
    "    logger.info(f\"Generating translations for {len(test_data)} samples\")\n",
    "    \n",
    "    for example in tqdm(test_data, desc=\"Generating translations\"):\n",
    "        input_text = example[\"tamil_sentence\"]\n",
    "        reference = example[\"telugu_sentence\"]\n",
    "        \n",
    "        try:\n",
    "            hypothesis = mbart_translate_text(input_text, debug=False)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error translating '{input_text[:50]}...': {e}\")\n",
    "            hypothesis = \"\"\n",
    "            \n",
    "        sources.append(input_text)\n",
    "        references.append(reference)\n",
    "        hypotheses.append(hypothesis)\n",
    "    \n",
    "    return test_data, sources, references, hypotheses\n",
    "\n",
    "# Compute BLEU score\n",
    "def compute_bleu(test_data, sources, references, hypotheses):\n",
    "    logger.info(\"Computing BLEU score...\")\n",
    "    \n",
    "    # Tokenize for BLEU calculation\n",
    "    tokenized_hypotheses = [indic_tokenize_text(hyp) for hyp in hypotheses]\n",
    "    tokenized_references = [[indic_tokenize_text(ref)] for ref in references]\n",
    "    \n",
    "    # Compute SacreBLEU score\n",
    "    bleu = corpus_bleu(tokenized_hypotheses, tokenized_references, tokenize='none')\n",
    "    bleu_score = bleu.score\n",
    "    logger.info(f\"BLEU Score: {bleu_score:.2f}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_df = pd.DataFrame({\n",
    "        \"tamil_sentence\": sources,\n",
    "        \"telugu_sentence\": references,\n",
    "        \"telugu_hypothesis\": hypotheses,\n",
    "        \"bleu_score\": [bleu_score] * len(sources)\n",
    "    })\n",
    "    results_df.to_csv(BLEU_OUTPUT_PATH, index=False)\n",
    "    logger.info(f\"BLEU results saved to {BLEU_OUTPUT_PATH}\")\n",
    "    \n",
    "    return bleu_score\n",
    "    \n",
    "# Compute chrF++ score\n",
    "def compute_chrf(test_data, sources, references, hypotheses):\n",
    "    logger.info(\"Computing chrF++ score...\")\n",
    "    \n",
    "    # Prepare references format for chrF++\n",
    "    refs_list = [[ref] for ref in references]\n",
    "    \n",
    "    # Compute chrF++ score\n",
    "    chrf = corpus_chrf(hypotheses, refs_list, char_order=6, word_order=2, beta=2)\n",
    "    chrf_score = chrf.score\n",
    "    logger.info(f\"chrF++ Score: {chrf_score:.2f}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_df = pd.DataFrame({\n",
    "        \"tamil_sentence\": sources,\n",
    "        \"telugu_sentence\": references,\n",
    "        \"telugu_hypothesis\": hypotheses,\n",
    "        \"chrf_score\": [chrf_score] * len(sources)\n",
    "    })\n",
    "    results_df.to_csv(CHRF_OUTPUT_PATH, index=False)\n",
    "    logger.info(f\"chrF++ results saved to {CHRF_OUTPUT_PATH}\")\n",
    "    \n",
    "    return chrf_score\n",
    "    \n",
    "# Compute TER score (Translation Edit Rate)\n",
    "def compute_ter(test_data, sources, references, hypotheses):\n",
    "    logger.info(\"Computing TER score...\")\n",
    "    \n",
    "    # Prepare references format for TER\n",
    "    refs_list = [[ref] for ref in references]\n",
    "    \n",
    "    # Compute TER score\n",
    "    ter = corpus_ter(hypotheses, refs_list)\n",
    "    ter_score = ter.score\n",
    "    logger.info(f\"TER Score: {ter_score:.2f}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_df = pd.DataFrame({\n",
    "        \"tamil_sentence\": sources,\n",
    "        \"telugu_sentence\": references,\n",
    "        \"telugu_hypothesis\": hypotheses,\n",
    "        \"ter_score\": [ter_score] * len(sources)\n",
    "    })\n",
    "    results_df.to_csv(TER_OUTPUT_PATH, index=False)\n",
    "    logger.info(f\"TER results saved to {TER_OUTPUT_PATH}\")\n",
    "    \n",
    "    return ter_score\n",
    "\n",
    "# Compute BERTScore\n",
    "def compute_bertscore(test_data, sources, references, hypotheses):\n",
    "    logger.info(\"Computing BERTScore...\")\n",
    "    \n",
    "    # Compute BERTScore\n",
    "    P, R, F1 = bert_score(\n",
    "        hypotheses,\n",
    "        references,\n",
    "        lang=\"te\",\n",
    "        model_type=\"bert-base-multilingual-cased\",\n",
    "        device=device,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Average F1 score\n",
    "    avg_f1 = F1.mean().item()\n",
    "    logger.info(f\"BERTScore F1: {avg_f1:.4f}\")\n",
    "    \n",
    "    # Store per-sentence F1 scores\n",
    "    bert_f1_scores = [f1.item() for f1 in F1]\n",
    "    \n",
    "    # Save results\n",
    "    results_df = pd.DataFrame({\n",
    "        \"tamil_sentence\": sources,\n",
    "        \"telugu_sentence\": references,\n",
    "        \"telugu_hypothesis\": hypotheses,\n",
    "        \"bertscore_f1\": bert_f1_scores\n",
    "    })\n",
    "    results_df.to_csv(BERTSCORE_OUTPUT_PATH, index=False)\n",
    "    logger.info(f\"BERTScore results saved to {BERTSCORE_OUTPUT_PATH}\")\n",
    "    \n",
    "    return avg_f1\n",
    "\n",
    "# Compute COMET score\n",
    "def compute_comet(test_data, sources, references, hypotheses):\n",
    "    if not comet_available:\n",
    "        logger.warning(\"COMET not available. Skipping COMET evaluation.\")\n",
    "        return None\n",
    "    \n",
    "    logger.info(\"Computing COMET score...\")\n",
    "    \n",
    "    # Load COMET model\n",
    "    logger.info(\"Downloading COMET model...\")\n",
    "    model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "    model = load_from_checkpoint(model_path)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Prepare data for COMET\n",
    "    data = []\n",
    "    for src, hyp, ref in zip(sources, hypotheses, references):\n",
    "        data.append({\n",
    "            \"src\": src,\n",
    "            \"mt\": hyp,\n",
    "            \"ref\": ref\n",
    "        })\n",
    "    \n",
    "    # Compute scores\n",
    "    logger.info(\"Running COMET evaluation...\")\n",
    "    model_output = model.predict(data, batch_size=8, gpus=1 if device == \"cuda\" else 0)\n",
    "    comet_scores = model_output.scores\n",
    "    avg_comet = model_output.system_score\n",
    "    \n",
    "    logger.info(f\"COMET Score: {avg_comet:.4f}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_df = pd.DataFrame({\n",
    "        \"tamil_sentence\": sources,\n",
    "        \"telugu_sentence\": references,\n",
    "        \"telugu_hypothesis\": hypotheses,\n",
    "        \"comet_score\": comet_scores\n",
    "    })\n",
    "    results_df.to_csv(COMET_OUTPUT_PATH, index=False)\n",
    "    logger.info(f\"COMET results saved to {COMET_OUTPUT_PATH}\")\n",
    "    \n",
    "    return avg_comet\n",
    "\n",
    "# Main evaluation function\n",
    "def evaluate_model():\n",
    "    # Generate translations\n",
    "    test_data, sources, references, hypotheses = generate_translations(test_dataset, NUM_SAMPLES)\n",
    "    \n",
    "    # Compute metrics\n",
    "    bleu_score = compute_bleu(test_data, sources, references, hypotheses)\n",
    "    bertscore_f1 = compute_bertscore(test_data, sources, references, hypotheses)\n",
    "    chrf_score = compute_chrf(test_data, sources, references, hypotheses)\n",
    "    ter_score = compute_ter(test_data, sources, references, hypotheses)\n",
    "    \n",
    "    comet_score = None\n",
    "    if comet_available:\n",
    "        comet_score = compute_comet(test_data, sources, references, hypotheses)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PARAGRAPH EVALUATION SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Number of samples: {len(sources)}\")\n",
    "    print(f\"BLEU Score: {bleu_score:.2f}\")\n",
    "    print(f\"chrF++ Score: {chrf_score:.2f}\")\n",
    "    print(f\"TER Score: {ter_score:.2f} (lower is better)\")\n",
    "    print(f\"BERTScore F1: {bertscore_f1:.4f}\")\n",
    "    if comet_score is not None:\n",
    "        print(f\"COMET Score: {comet_score:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Test a single paragraph translation\n",
    "    test_input = \"\"\"ஒவ்வொரு மனிதரும், ஒவ்வொரு வர்த்தகமும், ஏதாவது சிறப்பானவற்றை செய்வதற்கு, வரும் ஆண்டில் மேம்பாட்டிற்கு புத்தாண்டில் தீர்மானம் எடுத்துக் கொள்கிறது. பிரதமர் நரேந்திர மோடி 2021-ம் ஆண்டில் கடைசி மனதின் குரல் மூலம் பொதுமக்களுடன் உரையாடினார். தனிநபர்களின் நற்குணங்களை எடுத்துரைப்பதுடன், சமுதாயம் மற்றும் நாட்டு மக்களிடையே சிறப்பாக செயல்பட கடந்த ஏழு ஆண்டுகளாக அவரின் இந்த பயணம் எவ்வாறு ஊக்கமளிக்கிறது என்பதை அவர் எடுத்துக் கூறினார்.\"\"\"\n",
    "    translated_text = mbart_translate_text(test_input, debug=True)\n",
    "    print(f\"\\nTest Paragraph Translation:\")\n",
    "    print(f\"Source (Tamil):\\n{test_input}\")\n",
    "    print(f\"Target (Telugu):\\n{translated_text}\")\n",
    "    \n",
    "    return {\n",
    "        \"bleu\": bleu_score,\n",
    "        \"chrf\": chrf_score,\n",
    "        \"ter\": ter_score,\n",
    "        \"bertscore\": bertscore_f1,\n",
    "        \"comet\": comet_score\n",
    "    }\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
