{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd8d9d56-d703-4073-b036-10b7ab4e59b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in links: /tmp/tmpyc1fp0i3\n",
      "Requirement already satisfied: pip in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (25.0.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pip in /home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages (25.0.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of omegaconf: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: pip 25.0.1\n",
      "    Uninstalling pip-25.0.1:\n",
      "      Successfully uninstalled pip-25.0.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/mca/anaconda3/envs/nmt/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed pip-25.1.1\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (run this cell only if dependencies are not already installed)\n",
    "!python -m ensurepip --upgrade\n",
    "!python -m pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d4b873a-e9a8-4cb1-87f6-2b39162a92c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 14:57:42,869 - INFO - loading 1449338 words for fastText model from ./models/indicnlp.v1.ta.bin\n",
      "2025-05-05 14:57:49,711 - INFO - FastText lifecycle event {'params': 'FastText<vocab=0, vector_size=300, alpha=0.025>', 'datetime': '2025-05-05T14:57:49.711957', 'gensim': '4.3.3', 'python': '3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 17:29:18) [GCC 11.2.0]', 'platform': 'Linux-6.8.0-50-generic-x86_64-with-glibc2.35', 'event': 'created'}\n",
      "2025-05-05 14:57:49,712 - INFO - Updating model with new vocabulary\n",
      "2025-05-05 14:57:53,834 - INFO - FastText lifecycle event {'msg': 'added 1449338 new unique words (100.00% of original 1449338) and increased the count of 0 pre-existing words (0.00% of original 1449338)', 'datetime': '2025-05-05T14:57:53.834363', 'gensim': '4.3.3', 'python': '3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 17:29:18) [GCC 11.2.0]', 'platform': 'Linux-6.8.0-50-generic-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
      "2025-05-05 14:57:57,911 - INFO - deleting the raw counts dictionary of 1449338 items\n",
      "2025-05-05 14:57:57,912 - INFO - sample=0.0001 downsamples 306 most-common words\n",
      "2025-05-05 14:57:57,912 - INFO - FastText lifecycle event {'msg': 'downsampling leaves estimated 266585126.3542469 word corpus (71.6%% of prior 372501746)', 'datetime': '2025-05-05T14:57:57.912768', 'gensim': '4.3.3', 'python': '3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 17:29:18) [GCC 11.2.0]', 'platform': 'Linux-6.8.0-50-generic-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
      "2025-05-05 14:59:17,068 - INFO - FastText lifecycle event {'msg': 'loaded (3449338, 300) weight matrix for fastText model from ./models/indicnlp.v1.ta.bin', 'datetime': '2025-05-05T14:59:17.068136', 'gensim': '4.3.3', 'python': '3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 17:29:18) [GCC 11.2.0]', 'platform': 'Linux-6.8.0-50-generic-x86_64-with-glibc2.35', 'event': 'load_fasttext_format'}\n",
      "2025-05-05 14:59:17,138 - INFO - loading 720339 words for fastText model from ./models/indicnlp.v1.te.bin\n",
      "2025-05-05 14:59:20,452 - INFO - FastText lifecycle event {'params': 'FastText<vocab=0, vector_size=300, alpha=0.025>', 'datetime': '2025-05-05T14:59:20.452425', 'gensim': '4.3.3', 'python': '3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 17:29:18) [GCC 11.2.0]', 'platform': 'Linux-6.8.0-50-generic-x86_64-with-glibc2.35', 'event': 'created'}\n",
      "2025-05-05 14:59:20,453 - INFO - Updating model with new vocabulary\n",
      "2025-05-05 14:59:22,362 - INFO - FastText lifecycle event {'msg': 'added 720339 new unique words (100.00% of original 720339) and increased the count of 0 pre-existing words (0.00% of original 720339)', 'datetime': '2025-05-05T14:59:22.362043', 'gensim': '4.3.3', 'python': '3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 17:29:18) [GCC 11.2.0]', 'platform': 'Linux-6.8.0-50-generic-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
      "2025-05-05 14:59:24,372 - INFO - deleting the raw counts dictionary of 720339 items\n",
      "2025-05-05 14:59:24,372 - INFO - sample=0.0001 downsamples 318 most-common words\n",
      "2025-05-05 14:59:24,373 - INFO - FastText lifecycle event {'msg': 'downsampling leaves estimated 132841112.2454316 word corpus (66.3%% of prior 200477101)', 'datetime': '2025-05-05T14:59:24.373157', 'gensim': '4.3.3', 'python': '3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 17:29:18) [GCC 11.2.0]', 'platform': 'Linux-6.8.0-50-generic-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
      "2025-05-05 14:59:58,577 - INFO - FastText lifecycle event {'msg': 'loaded (2720339, 300) weight matrix for fastText model from ./models/indicnlp.v1.te.bin', 'datetime': '2025-05-05T14:59:58.577473', 'gensim': '4.3.3', 'python': '3.12.0 | packaged by Anaconda, Inc. | (main, Oct  2 2023, 17:29:18) [GCC 11.2.0]', 'platform': 'Linux-6.8.0-50-generic-x86_64-with-glibc2.35', 'event': 'load_fasttext_format'}\n",
      "2025-05-05 14:59:59,497 - INFO - Cleaning corpus: ./Finance_Data/Finance_Data(TAM).csv\n",
      "Processing: 4it [00:01,  2.61it/s]\n",
      "2025-05-05 15:00:01,204 - INFO - Total pairs: 18644, Clean pairs: 17618, Problematic: 1026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned corpus saved to: ./Finance_Data/./Finance_Data/FastText/cleaned_corpus.csv\n",
      "Problematic pairs saved to: ./Finance_Data/./Finance_Data/FastText/problematic_pairs.csv\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "from typing import List, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CorpusVerifierCSV:\n",
    "    def __init__(self, input_csv: str, output_dir: str = None, batch_size: int = 5000,\n",
    "                 tamil_col: str = \"Tamil\", telugu_col: str = \"Telugu\"):\n",
    "        \"\"\"\n",
    "        Initialize the CorpusVerifierCSV class.\n",
    "        \n",
    "        Args:\n",
    "            input_csv (str): Path to the input CSV file.\n",
    "            output_dir (str, optional): Directory to save output files. Defaults to a folder named 'cleaned_output' in the input CSV's directory.\n",
    "            batch_size (int): Number of rows to process per chunk.\n",
    "            tamil_col (str): Name of the column containing Tamil text.\n",
    "            telugu_col (str): Name of the column containing Telugu text.\n",
    "        \"\"\"\n",
    "        self.input_csv = input_csv\n",
    "        self.batch_size = batch_size\n",
    "        self.tamil_col = tamil_col\n",
    "        self.telugu_col = telugu_col\n",
    "        self.output_dir = output_dir or os.path.join(os.path.dirname(input_csv), \"./Finance_Data/FastText\")\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "        # Load IndicNLP FastText embeddings\n",
    "        # Update these paths to point to your local FastText model files\n",
    "        self.tamil_embeddings = load_facebook_model(\"./models/indicnlp.v1.ta.bin\")\n",
    "        self.telugu_embeddings = load_facebook_model(\"./models/indicnlp.v1.te.bin\")\n",
    "        self.tamil_vocab = set(self.tamil_embeddings.wv.index_to_key)  # ~9.45M word types\n",
    "        self.telugu_vocab = set(self.telugu_embeddings.wv.index_to_key)  # ~4.19M word types\n",
    "\n",
    "        self.tamil_script_pattern = re.compile(r'[\\u0B80-\\u0BFF]')\n",
    "        self.telugu_script_pattern = re.compile(r'[\\u0C00-\\u0C7F]')\n",
    "\n",
    "    def _ensure_utf8(self, text: str) -> str:\n",
    "        \"\"\"Ensure text is UTF-8 encoded.\"\"\"\n",
    "        return \"\" if pd.isna(text) else text.encode().decode('utf-8', errors='replace')\n",
    "\n",
    "    def _check_vocabulary_baseline(self, text: str, lang: str) -> float:\n",
    "        \"\"\"Check proportion of tokens in IndicNLP vocabulary.\"\"\"\n",
    "        text = self._ensure_utf8(text)\n",
    "        if not text.strip():\n",
    "            return 0.0\n",
    "\n",
    "        tokens = indic_tokenize.trivial_tokenize(text, lang=lang)\n",
    "        if not tokens:\n",
    "            return 0.0\n",
    "\n",
    "        vocab = self.tamil_vocab if lang == 'ta' else self.telugu_vocab\n",
    "        in_vocab_count = sum(1 for token in tokens if token in vocab)\n",
    "        return in_vocab_count / len(tokens)  # Proportion of valid words\n",
    "\n",
    "    def _verify_chunk(self, df_chunk: pd.DataFrame) -> List[Dict]:\n",
    "        \"\"\"Verify a chunk of the DataFrame for problematic entries.\"\"\"\n",
    "        problems = []\n",
    "        for idx, row in df_chunk.iterrows():\n",
    "            tamil_text = row[self.tamil_col]\n",
    "            telugu_text = row[self.telugu_col]\n",
    "\n",
    "            if pd.isna(tamil_text) or pd.isna(telugu_text):\n",
    "                problems.append({'index': idx, 'Tamil': tamil_text, 'Telugu': telugu_text, 'reason': 'missing_values'})\n",
    "                continue\n",
    "\n",
    "            tamil_text = self._ensure_utf8(tamil_text)\n",
    "            telugu_text = self._ensure_utf8(telugu_text)\n",
    "\n",
    "            tamil_vocab_score = self._check_vocabulary_baseline(tamil_text, 'ta')\n",
    "            telugu_vocab_score = self._check_vocabulary_baseline(telugu_text, 'te')\n",
    "\n",
    "            reasons = []\n",
    "            if tamil_vocab_score < 0.75:\n",
    "                reasons.append('tamil_low_vocab_coverage')\n",
    "            if telugu_vocab_score < 0.75:\n",
    "                reasons.append('telugu_low_vocab_coverage')\n",
    "\n",
    "            if reasons:\n",
    "                problems.append({\n",
    "                    'index': idx,\n",
    "                    'Tamil': tamil_text,\n",
    "                    'Telugu': telugu_text,\n",
    "                    'tamil_vocab_score': tamil_vocab_score,\n",
    "                    'telugu_vocab_score': telugu_vocab_score,\n",
    "                    'reason': ','.join(reasons)\n",
    "                })\n",
    "        return problems\n",
    "\n",
    "    def run_verification(self):\n",
    "        \"\"\"Run verification on the entire corpus and save results.\"\"\"\n",
    "        logger.info(f\"Cleaning corpus: {self.input_csv}\")\n",
    "        all_problems = []\n",
    "        total_rows = 0\n",
    "\n",
    "        for df_chunk in tqdm(pd.read_csv(self.input_csv, chunksize=self.batch_size, encoding='utf-8'), desc=\"Processing\"):\n",
    "            total_rows += len(df_chunk)\n",
    "            problems = self._verify_chunk(df_chunk)\n",
    "            all_problems.extend(problems)\n",
    "\n",
    "        full_df = pd.read_csv(self.input_csv, encoding='utf-8')\n",
    "        problem_indices = set(p['index'] for p in all_problems)\n",
    "        clean_df = full_df[~full_df.index.isin(problem_indices)]\n",
    "\n",
    "        clean_path = os.path.join(self.output_dir, 'cleaned_corpus.csv')\n",
    "        problem_path = os.path.join(self.output_dir, 'problematic_pairs.csv')\n",
    "        clean_df.to_csv(clean_path, index=False, encoding='utf-8')\n",
    "        pd.DataFrame(all_problems).to_csv(problem_path, index=False, encoding='utf-8')\n",
    "\n",
    "        logger.info(f\"Total pairs: {total_rows}, Clean pairs: {len(clean_df)}, Problematic: {len(all_problems)}\")\n",
    "        print(f\"Cleaned corpus saved to: {clean_path}\")\n",
    "        print(f\"Problematic pairs saved to: {problem_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Update this path to your local CSV file\n",
    "    input_csv_path = \"./Finance_Data/Finance_Data(TAM).csv\"\n",
    "    verifier = CorpusVerifierCSV(input_csv=input_csv_path)\n",
    "    verifier.run_verification()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
